diff -urN --no-dereference gcc-clean/gcc/config/xr17032/constraints.md gcc-workdir/gcc/config/xr17032/constraints.md
--- gcc-clean/gcc/config/xr17032/constraints.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/constraints.md
@@ -0,0 +1,58 @@
+;; Constraint definitions for XR/17032
+;; Copyright (C) 2025-2025 Free Software Foundation, Inc.
+;; Contributed by monkuous
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_constraint "I" "Unsigned 16-bit immediate"
+  (and (match_code "const_int")
+       (match_test "SMALL_OPERAND (ival)")))
+
+(define_constraint "J" "Signed 16-bit immediate"
+  (and (match_code "const_int")
+       (match_test "SMALL_OPERAND_SIGNED (ival)")))
+
+(define_constraint "L" "Unsigned 5-bit immediate"
+  (and (match_code "const_int")
+       (match_test "(unsigned HOST_WIDE_INT) (ival) < 32")))
+
+(define_constraint "O" "Constant zero"
+  (and (match_code "const_int")
+       (match_test "ival == 0")))
+
+(define_constraint "Sa"
+  "A constraint that matches an absolute symbolic address."
+  (match_operand 0 "xr17032_absolute_symbolic_operand"))
+
+(define_constraint "Sp"
+  "A constraint that matches a PC-relative symbolic address."
+  (match_operand 0 "xr17032_pcrel_symbolic_operand"))
+
+(define_constraint "Sg"
+  "A constraint that matches a GOT-indirect symbolic address."
+  (match_operand 0 "xr17032_got_symbolic_operand"))
+
+(define_constraint "T"
+  "@internal
+   A constant @code{move_operand}."
+  (and (match_operand 0 "xr17032_move_operand")
+       (match_test "CONSTANT_P (op)")))
+
+(define_memory_constraint "A"
+  "Memory operand whose address is a single register"
+  (and (match_code "mem")
+       (match_test "GET_CODE (XEXP (op, 0)) == REG")))
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/linux.h gcc-workdir/gcc/config/xr17032/linux.h
--- gcc-clean/gcc/config/xr17032/linux.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/linux.h
@@ -0,0 +1,41 @@
+/* Definitions for XR/17032 GNU/Linux systems with ELF format.
+   Copyright (C) 2025-2025 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 3, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+#define TARGET_OS_CPP_BUILTINS()				\
+  do {								\
+    GNU_USER_TARGET_OS_CPP_BUILTINS();				\
+  } while (0)
+
+#define GLIBC_DYNAMIC_LINKER "/lib/ld-linux.so.1"
+
+#define CPP_SPEC "%{pthread:-D_REENTRANT}"
+
+#define LINK_SPEC "\
+-X \
+%{shared} \
+  %{!shared: \
+    %{!static: \
+      %{!static-pie: \
+	%{rdynamic:-export-dynamic} \
+	-dynamic-linker " GNU_USER_DYNAMIC_LINKER "}} \
+    %{static:-static} %{static-pie:-static -pie --no-dynamic-linker -z text}}"
+
+#define STARTFILE_PREFIX_SPEC 			\
+   "/lib/ "					\
+   "/usr/lib/ "
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/predicates.md gcc-workdir/gcc/config/xr17032/predicates.md
--- gcc-clean/gcc/config/xr17032/predicates.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/predicates.md
@@ -0,0 +1,193 @@
+;; Predicate description for XR/17032
+;; Copyright (C) 2025-2025 Free Software Foundation, Inc.
+;; Contributed by monkuous
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_predicate "xr17032_reg_shift_operand"
+  (ior (match_operand 0 "register_operand")
+       (and (match_code "mult")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "pow2p_hwi (INTVAL (XEXP (op, 1)))")
+	    (match_test "IN_RANGE (exact_log2 (INTVAL (XEXP(op, 1))), 0, 31)"))
+       (and (match_code "div")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "pow2p_hwi (INTVAL (XEXP (op, 1)))")
+	    (match_test "IN_RANGE (exact_log2 (INTVAL (XEXP(op, 1))), 0, 31)"))
+       (and (match_code "ashift")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "IN_RANGE (INTVAL (XEXP (op, 1)), 0, 31)"))
+       (and (match_code "rotate")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "IN_RANGE (INTVAL (XEXP (op, 1)), 0, 31)"))
+       (and (match_code "ashiftrt")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "IN_RANGE (INTVAL (XEXP (op, 1)), 0, 31)"))
+       (and (match_code "lshiftrt")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "IN_RANGE (INTVAL (XEXP (op, 1)), 0, 31)"))
+       (and (match_code "rotatert")
+	    (match_test "register_operand (XEXP (op, 0), Pmode)")
+	    (match_test "CONST_INT_P (XEXP (op, 1))")
+	    (match_test "IN_RANGE (INTVAL (XEXP (op, 1)), 0, 31)"))))
+
+(define_predicate "xr17032_reg_or_0_operand"
+  (ior (match_operand 0 "register_operand")
+       (and (match_code "const_int")
+	    (match_test "INTVAL (op) == 0"))))
+
+(define_predicate "xr17032_smallimm_operand"
+  (and (match_code "const_int")
+       (match_test "SMALL_OPERAND (INTVAL (op))")))
+
+(define_predicate "xr17032_smallimm_operand_signed"
+  (and (match_code "const_int")
+       (match_test "SMALL_OPERAND_SIGNED (INTVAL (op))")))
+
+(define_predicate "xr17032_move_operand"
+  (match_operand 0 "general_operand")
+{
+  enum xr17032_symbol_type symbol_type;
+
+  /* The thinking here is as follows:
+
+     (1) The move expanders should split complex load sequences into
+	 individual instructions.  Those individual instructions can
+	 then be optimized by all rtl passes.
+
+     (2) The target of pre-reload load sequences should not be used
+	 to store temporary results.  If the target register is only
+	 assigned one value, reload can rematerialize that value
+	 on demand, rather than spill it to the stack.
+
+     (3) If we allowed pre-reload passes like combine and cse to recreate
+	 complex load sequences, we would want to be able to split the
+	 sequences before reload as well, so that the pre-reload scheduler
+	 can see the individual instructions.  This falls foul of (2);
+	 the splitter would be forced to reuse the target register for
+	 intermediate results.
+
+     (4) We want to define complex load splitters for combine.  These
+	 splitters can request a temporary scratch register, which avoids
+	 the problem in (2).  They allow things like:
+
+	      (set (reg T1) (high SYM))
+	      (set (reg T2) (low (reg T1) SYM))
+	      (set (reg X) (plus (reg T2) (const_int OFFSET)))
+
+	 to be combined into:
+
+	      (set (reg T3) (high SYM+OFFSET))
+	      (set (reg X) (lo_sum (reg T3) SYM+OFFSET))
+
+	 if T2 is only used this once.  */
+  switch (GET_CODE (op))
+    {
+    case CONST_INT:
+      return !xr17032_splittable_const_int_operand (op, mode);
+
+    case CONST:
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return xr17032_symbolic_constant_p (op, &symbol_type)
+	     && !xr17032_split_symbol_type (symbol_type);
+
+    case HIGH:
+      op = XEXP (op, 0);
+      return xr17032_symbolic_constant_p (op, &symbol_type)
+	      && xr17032_split_symbol_type (symbol_type)
+	      && symbol_type != XR17032_SYMBOL_PCREL;
+
+    default:
+      return true;
+    }
+})
+
+(define_predicate "xr17032_arith_operand"
+  (ior (match_operand 0 "register_operand")
+       (match_operand 0 "xr17032_smallimm_operand")))
+
+(define_predicate "xr17032_arith_operand_signed"
+  (ior (match_operand 0 "register_operand")
+       (match_operand 0 "xr17032_smallimm_operand_signed")))
+
+(define_predicate "xr17032_absolute_symbolic_operand"
+  (match_code "const,symbol_ref,label_ref")
+{
+  enum xr17032_symbol_type type;
+  return (xr17032_symbolic_constant_p (op, &type)
+	  && (type == XR17032_SYMBOL_ABSOLUTE));
+})
+
+(define_predicate "xr17032_pcrel_symbolic_operand"
+  (match_code "const,symbol_ref,label_ref")
+{
+  enum xr17032_symbol_type type;
+  return (xr17032_symbolic_constant_p (op, &type)
+	  && (type == XR17032_SYMBOL_PCREL));
+})
+
+(define_predicate "xr17032_got_symbolic_operand"
+  (match_code "const,symbol_ref,label_ref")
+{
+  enum xr17032_symbol_type type;
+  return (xr17032_symbolic_constant_p (op, &type)
+	  && (type == XR17032_SYMBOL_GOT_DISP));
+})
+
+(define_predicate "xr17032_slt_operator"
+  (match_code "lt,ltu"))
+
+(define_predicate "xr17032_shift_operand"
+  (ior (match_operand 0 "register_operand")
+       (and (match_code "const_int")
+	    (match_test "(unsigned HOST_WIDE_INT) INTVAL (op) < 32"))))
+
+(define_predicate "xr17032_call_operand"
+  (ior (match_operand 0 "register_operand")
+       (match_operand 0 "xr17032_absolute_symbolic_operand")
+       (match_operand 0 "xr17032_pcrel_symbolic_operand")
+       (match_operand 0 "xr17032_got_symbolic_operand")))
+
+(define_predicate "xr17032_symbolic_operand"
+  (match_code "const,symbol_ref,label_ref")
+{
+  enum xr17032_symbol_type type;
+  return xr17032_symbolic_constant_p (op, &type);
+})
+
+;; A legitimate CONST_INT operand that takes more than one instruction
+;; to load.
+(define_predicate "xr17032_splittable_const_int_operand"
+  (match_code "const_int")
+{
+  /* Don't handle multi-word moves this way; we don't want to introduce
+     the individual word-mode moves until after reload.  */
+  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+    return false;
+
+  /* Otherwise check whether the constant can be loaded in a single
+     instruction.  */
+  return !LUI_OPERAND (INTVAL (op)) && !SMALL_OPERAND (INTVAL (op))
+         && !SMALL_OPERAND (-INTVAL (op));
+})
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/sync.md gcc-workdir/gcc/config/xr17032/sync.md
--- gcc-clean/gcc/config/xr17032/sync.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/sync.md
@@ -0,0 +1,626 @@
+;; Machine description for XR/17032 atomic operations
+;; Copyright (C) 2025-2025 Free Software Foundation, Inc.
+;; Contributed by monkuous
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_c_enum "unspec" [
+  UNSPEC_COMPARE_AND_SWAP
+  UNSPEC_COMPARE_AND_SWAP_SUBWORD
+  UNSPEC_SYNC_OLD_OP
+  UNSPEC_SYNC_OLD_OP_SUBWORD
+  UNSPEC_SYNC_EXCHANGE
+  UNSPEC_SYNC_EXCHANGE_SUBWORD
+  UNSPEC_ATOMIC_LOAD
+  UNSPEC_ATOMIC_STORE
+  UNSPEC_MEMORY_BARRIER
+])
+
+;; Memory barriers.
+
+(define_expand "mem_thread_fence"
+  [(match_operand:SI 0 "const_int_operand" "")] ;; model
+  ""
+  {
+    enum memmodel model = memmodel_base (INTVAL (operands[0]));
+
+    if (model != MEMMODEL_RELAXED)
+      {
+	rtx mem = gen_rtx_MEM (BLKmode, gen_rtx_SCRATCH (Pmode));
+	MEM_VOLATILE_P (mem) = 1;
+	emit_insn (gen_mem_thread_fence_internal (mem, operands[0]));
+      }
+    DONE;
+  })
+
+(define_insn "mem_thread_fence_internal"
+  [(set (match_operand:BLK 0 "" "")
+	(unspec:BLK [(match_dup 0)] UNSPEC_MEMORY_BARRIER))
+   (match_operand:SI 1 "const_int_operand" "")]  ;; model
+  ""
+  {
+    enum memmodel model = (enum memmodel) INTVAL (operands[1]);
+    model = memmodel_base (model);
+
+    if (model == MEMMODEL_SEQ_CST || model == MEMMODEL_ACQ_REL
+	|| model == MEMMODEL_ACQUIRE)
+	return "mb";
+    else if (model == MEMMODEL_RELEASE)
+	return "wmb";
+    else
+	gcc_unreachable ();
+  }
+  [(set (attr "length") (const_int 4))])
+
+(define_insn "atomic_load<mode>"
+  [(set (match_operand:ANYI 0 "register_operand" "=r")
+	(unspec_volatile:ANYI
+	    [(match_operand:ANYI 1 "memory_operand" "m")
+	     (match_operand:SI 2 "const_int_operand")]  ;; model
+	 UNSPEC_ATOMIC_LOAD))]
+  ""
+  {
+    enum memmodel model = (enum memmodel) INTVAL (operands[2]);
+    model = memmodel_base (model);
+
+    if (model == MEMMODEL_SEQ_CST)
+      return "mb\;"
+	     "mov\t%0,%1\;"
+	     "mb";
+    if (model == MEMMODEL_ACQUIRE)
+      return "mov\t%0,%1\;"
+	     "mb";
+    else
+      return "mov\t%0,%1";
+  }
+  [(set (attr "length")
+	(symbol_ref "(is_mm_seq_cst (memmodel_from_int (INTVAL (operands[2]))) ? 12
+		      : is_mm_acquire (memmodel_from_int (INTVAL (operands[2]))) ? 8
+		      : 4)"))])
+
+(define_insn "atomic_store<mode>"
+  [(set (match_operand:ANYI 0 "memory_operand" "=A")
+	(unspec_volatile:ANYI
+	    [(match_operand:ANYI 1 "xr17032_reg_or_0_operand" "rO")
+	     (match_operand:SI 2 "const_int_operand")]  ;; model
+	 UNSPEC_ATOMIC_STORE))]
+  ""
+  {
+    enum memmodel model = (enum memmodel) INTVAL (operands[2]);
+    model = memmodel_base (model);
+
+    if (model == MEMMODEL_SEQ_CST)
+      return "wmb\;"
+	     "mov\t%0,%z1\;"
+	     "mb";
+    if (model == MEMMODEL_RELEASE)
+      return "wmb\;"
+	     "mov\t%0,%z1";
+    else
+      return "mov\t%0,%z1";
+  }
+  [(set (attr "length")
+	(symbol_ref "(is_mm_seq_cst (memmodel_from_int (INTVAL (operands[2]))) ? 12
+		      : is_mm_release (memmodel_from_int (INTVAL (operands[2]))) ? 8
+		      : 4)"))])
+
+(define_expand "atomic_<atomic_optab>si"
+  [(match_operand:SI 0 "memory_operand"   "+A")
+   (any_atomic:SI (match_dup 0)
+                  (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO"))
+   (match_operand:SI 2 "const_int_operand")]
+  ""
+  {
+    emit_insn (gen_mem_thread_fence (operands[2]));
+    emit_insn (gen_atomic_<atomic_optab>si_internal (operands[0], operands[1]));
+    emit_insn (gen_mem_thread_fence (operands[2]));
+    DONE;
+  })
+
+(define_insn "atomic_<atomic_optab>si_internal"
+  [(set (match_operand:SI 0 "memory_operand" "+A")
+	(unspec_volatile:SI
+	  [(any_atomic:SI (match_dup 0)
+		          (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO"))]
+	 UNSPEC_SYNC_OLD_OP))
+   (clobber (match_scratch:SI 2 "=&r"))]	     ;; tmp_1
+  ""
+  {
+    return "1:\;"
+	   "ll\t%2,%A0\;"
+	   "<insn>\t%2,%2,%z1\;"
+	   "sc\t%2,%A0,%2\;"
+	   "beq\t%2,1b";
+  }
+  [(set (attr "length") (const_int 16))])
+
+;; AMO fetch ops
+
+(define_expand "atomic_fetch_<atomic_optab>si"
+  [(match_operand:SI 0 "register_operand")		     ;; old value at mem
+   (any_atomic:SI (match_operand:SI 1 "memory_operand")    ;; mem location
+		  (match_operand:SI 2 "xr17032_reg_or_0_operand")) ;; value for op
+   (match_operand:SI 3 "const_int_operand")]		     ;; model
+  ""
+  {
+    emit_insn (gen_mem_thread_fence (operands[3]));
+    emit_insn (gen_atomic_fetch_<atomic_optab>si_internal (operands[0],
+							   operands[1],
+							   operands[2]));
+    emit_insn (gen_mem_thread_fence (operands[3]));
+    DONE;
+  })
+
+(define_insn "atomic_fetch_<atomic_optab>si_internal"
+  [(set (match_operand:SI 0 "register_operand" "=&r")
+	(match_operand:SI 1 "memory_operand" "+A"))
+   (set (match_dup 1)
+	(unspec_volatile:SI
+	  [(any_atomic:SI (match_dup 1)
+			  (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO"))]
+	 UNSPEC_SYNC_OLD_OP))
+   (clobber (match_scratch:SI 3 "=&r"))]	  ;; tmp_1
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "<insn>\t%3,%0,%z2\;"
+	   "sc\t%3,%A1,%3\;"
+	   "beq\t%3,1b";
+  }
+  [(set (attr "length") (const_int 16))])
+
+(define_insn "subword_atomic_fetch_strong_<atomic_optab>"
+  [(set (match_operand:SI 0 "register_operand" "=&r")		    ;; old value at mem
+	(match_operand:SI 1 "memory_operand" "+A"))		    ;; mem location
+   (set (match_dup 1)
+	(unspec_volatile:SI
+	  [(any_atomic:SI (match_dup 1)
+		     (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO"))] ;; value for op
+	 UNSPEC_SYNC_OLD_OP_SUBWORD))
+    (match_operand:SI 3 "register_operand" "rI")		    ;; mask
+    (match_operand:SI 4 "register_operand" "rI")		    ;; not_mask
+    (clobber (match_scratch:SI 5 "=&r"))			    ;; tmp_1
+    (clobber (match_scratch:SI 6 "=&r"))]			    ;; tmp_2
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "<insn>\t%5,%0,%z2\;"
+	   "and%i3\t%5,%5,%3\;"
+	   "and%i4\t%6,%0,%4\;"
+	   "or\t%6,%6,%5\;"
+	   "sc\t%5,%A1,%6\;"
+	   "beq\t%5,1b";
+  }
+  [(set (attr "length") (const_int 28))])
+
+(define_expand "atomic_fetch_nand<mode>"
+  [(match_operand:QIHI 0 "register_operand")			      ;; old value at mem
+   (not:QIHI (and:QIHI (match_operand:QIHI 1 "memory_operand")     ;; mem location
+                       (match_operand:QIHI 2 "xr17032_reg_or_0_operand"))) ;; value for op
+   (match_operand:SI 3 "const_int_operand")]			      ;; model
+  ""
+{
+  /* We have no QImode/HImode atomics, so form a mask, then use
+     subword_atomic_fetch_strong_nand to implement a LL/SC version of the
+     operation.  */
+
+  rtx old = gen_reg_rtx (SImode);
+  rtx mem = operands[1];
+  rtx value = operands[2];
+  rtx model = operands[3];
+  rtx aligned_mem = gen_reg_rtx (SImode);
+  rtx shift = gen_reg_rtx (SImode);
+  rtx mask = gen_reg_rtx (SImode);
+  rtx not_mask = gen_reg_rtx (SImode);
+
+  xr17032_subword_address (mem, &aligned_mem, &shift, &mask, &not_mask);
+
+  rtx shifted_value = gen_reg_rtx (SImode);
+  xr17032_lshift_subword (<MODE>mode, value, shift, &shifted_value);
+
+  emit_insn (gen_mem_thread_fence (model));
+  emit_insn (gen_subword_atomic_fetch_strong_nand (old, aligned_mem,
+						   shifted_value, mask,
+						   not_mask));
+  emit_insn (gen_mem_thread_fence (model));
+
+  emit_move_insn (old, gen_rtx_ASHIFTRT (SImode, old, shift));
+
+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, old));
+
+  DONE;
+})
+
+(define_insn "subword_atomic_fetch_strong_nand"
+  [(set (match_operand:SI 0 "register_operand" "=&r")			   ;; old value at mem
+	(match_operand:SI 1 "memory_operand" "+A"))			   ;; mem location
+   (set (match_dup 1)
+	(unspec_volatile:SI
+	  [(not:SI (and:SI (match_dup 1)
+			   (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")))] ;; value for op
+	 UNSPEC_SYNC_OLD_OP_SUBWORD))
+    (match_operand:SI 3 "register_operand" "rI")			   ;; mask
+    (match_operand:SI 4 "register_operand" "rI")			   ;; not_mask
+    (clobber (match_scratch:SI 5 "=&r"))				   ;; tmp_1
+    (clobber (match_scratch:SI 6 "=&r"))]				   ;; tmp_2
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "and\t%5,%0,%z2\;"
+	   "nor\t%5,%5,zero\;"
+	   "and%i3\t%5,%5,%3\;"
+	   "and%i4\t%5,%0,%4\;"
+	   "or\t%6,%6,%5\;"
+	   "sc\t%5,%A1,%6\;"
+	   "beq\t%5,1b";
+  }
+  [(set (attr "length") (const_int 32))])
+
+(define_expand "atomic_fetch_<atomic_optab><mode>"
+  [(match_operand:QIHI 0 "register_operand")			 ;; old value at mem
+   (any_atomic:QIHI (match_operand:QIHI 1 "memory_operand")	 ;; mem location
+		    (match_operand:QIHI 2 "xr17032_reg_or_0_operand")) ;; value for op
+   (match_operand:SI 3 "const_int_operand")]			 ;; model
+  ""
+{
+  /* We have no QImode/HImode atomics, so form a mask, then use
+     subword_atomic_fetch_strong_<mode> to implement a LL/SC version of the
+     operation.  */
+
+  rtx old = gen_reg_rtx (SImode);
+  rtx mem = operands[1];
+  rtx value = operands[2];
+  rtx model = operands[3];
+  rtx aligned_mem = gen_reg_rtx (SImode);
+  rtx shift = gen_reg_rtx (SImode);
+  rtx mask = gen_reg_rtx (SImode);
+  rtx not_mask = gen_reg_rtx (SImode);
+
+  xr17032_subword_address (mem, &aligned_mem, &shift, &mask, &not_mask);
+
+  rtx shifted_value = gen_reg_rtx (SImode);
+  xr17032_lshift_subword (<MODE>mode, value, shift, &shifted_value);
+
+  emit_insn (gen_mem_thread_fence (model));
+  emit_insn (gen_subword_atomic_fetch_strong_<atomic_optab> (old, aligned_mem,
+							     shifted_value,
+							     mask, not_mask));
+  emit_insn (gen_mem_thread_fence (model));
+
+  emit_move_insn (old, gen_rtx_ASHIFTRT (SImode, old, shift));
+
+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, old));
+
+  DONE;
+})
+
+; Atomic exchange ops
+
+(define_expand "atomic_exchangesi"
+  [(match_operand:SI 0 "register_operand")   ;; old value at mem
+   (match_operand:SI 1 "memory_operand")     ;; mem location
+   (match_operand:SI 2 "register_operand")   ;; value for op
+   (match_operand:SI 3 "const_int_operand")] ;; model
+  ""
+  {
+    emit_insn (gen_mem_thread_fence (operands[3]));
+    emit_insn (gen_atomic_exchangesi_internal (operands[0], operands[1],
+					       operands[2]));
+    emit_insn (gen_mem_thread_fence (operands[3]));
+    DONE;
+  })
+
+(define_insn "atomic_exchangesi_internal"
+  [(set (match_operand:SI 0 "register_operand" "=&r")
+	(unspec_volatile:SI
+	  [(match_operand:SI 1 "memory_operand" "+A")]
+	  UNSPEC_SYNC_EXCHANGE))
+   (set (match_dup 1)
+	(match_operand:SI 2 "register_operand" "0"))
+   (clobber (match_scratch:SI 3 "=&r"))]	  ;; tmp_1
+  ""
+  {
+    return "1:\;"
+	   "ll\t%3,%A1\;"
+	   "sc\t%0,%A1,%0\;"
+	   "beq\t%0,1b\;"
+	   "add\t%0,%3,zero";
+  }
+  [(set (attr "length") (const_int 16))])
+
+(define_expand "atomic_exchange<mode>"
+  [(match_operand:QIHI 0 "register_operand") ;; old value at mem
+   (match_operand:QIHI 1 "memory_operand")   ;; mem location
+   (match_operand:QIHI 2 "register_operand") ;; value
+   (match_operand:SI 3 "const_int_operand")]  ;; model
+  ""
+{
+  rtx old = gen_reg_rtx (SImode);
+  rtx mem = operands[1];
+  rtx value = operands[2];
+  rtx model = operands[3];
+  rtx aligned_mem = gen_reg_rtx (SImode);
+  rtx shift = gen_reg_rtx (SImode);
+  rtx mask = gen_reg_rtx (SImode);
+  rtx not_mask = gen_reg_rtx (SImode);
+
+  xr17032_subword_address (mem, &aligned_mem, &shift, &mask, &not_mask);
+
+  rtx shifted_value = gen_reg_rtx (SImode);
+  xr17032_lshift_subword (<MODE>mode, value, shift, &shifted_value);
+  emit_move_insn (shifted_value, gen_rtx_AND (SImode, shifted_value, mask));
+
+  emit_insn (gen_mem_thread_fence (model));
+  emit_insn (gen_subword_atomic_exchange_strong (old, aligned_mem,
+						 shifted_value, not_mask));
+  emit_insn (gen_mem_thread_fence (model));
+
+  emit_move_insn (old, gen_rtx_ASHIFTRT (SImode, old, shift));
+
+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, old));
+  DONE;
+})
+
+(define_insn "subword_atomic_exchange_strong"
+  [(set (match_operand:SI 0 "register_operand" "=&r")	 ;; old value at mem
+	(match_operand:SI 1 "memory_operand" "+A"))	 ;; mem location
+   (set (match_dup 1)
+	(unspec_volatile:SI
+	  [(match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")] ;; value
+      UNSPEC_SYNC_EXCHANGE_SUBWORD))
+    (match_operand:SI 3 "xr17032_reg_or_0_operand" "rO")	 ;; not_mask
+    (clobber (match_scratch:SI 4 "=&r"))]		 ;; tmp_1
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "and\t%4,%0,%z3\;"
+	   "or\t%4,%4,%z2\;"
+	   "sc\t%4,%A1,%4\;"
+	   "beq\t%4,1b";
+  }
+  [(set (attr "length") (const_int 20))])
+
+; Atomic CAS ops
+
+(define_expand "atomic_cas_value_strongsi"
+  [(match_operand:SI 0 "register_operand")
+   (match_operand:SI 1 "memory_operand")
+   (match_operand:SI 2 "xr17032_reg_or_0_operand")
+   (match_operand:SI 3 "xr17032_reg_or_0_operand")
+   (match_operand:SI 4 "const_int_operand")  ;; mod_s
+   (match_operand:SI 5 "const_int_operand")] ;; mod_f
+  ""
+  {
+    emit_insn (gen_mem_thread_fence (operands[5]));
+    emit_insn (gen_atomic_cas_value_strongsi_internal (operands[0], operands[1],
+						       operands[2], operands[3]));
+    emit_insn (gen_mem_thread_fence (operands[5]));
+    DONE;
+  })
+
+(define_insn "atomic_cas_value_strongsi_internal"
+  [(set (match_operand:SI 0 "register_operand" "=&r")
+	(match_operand:SI 1 "memory_operand" "+A"))
+   (set (match_dup 1)
+	(unspec_volatile:SI [(match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")
+			     (match_operand:SI 3 "xr17032_reg_or_0_operand" "rO")]
+	 UNSPEC_COMPARE_AND_SWAP))
+   (clobber (match_scratch:SI 4 "=&r"))]
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "sub\t%4,%0,%z2\;"
+	   "bne\t%4,1f\;"
+	   "sc\t%4,%A1,%z3\;"
+	   "beq\t%4,1b\;"
+	   "1:";
+  }
+  [(set (attr "length") (const_int 20))])
+
+(define_expand "atomic_compare_and_swapsi"
+  [(match_operand:SI 0 "register_operand" "")   ;; bool output
+   (match_operand:SI 1 "register_operand" "")  ;; val output
+   (match_operand:SI 2 "memory_operand" "")    ;; memory
+   (match_operand:SI 3 "register_operand" "")  ;; expected value
+   (match_operand:SI 4 "xr17032_reg_or_0_operand" "")  ;; desired value
+   (match_operand:SI 5 "const_int_operand" "")  ;; is_weak
+   (match_operand:SI 6 "const_int_operand" "")  ;; mod_s
+   (match_operand:SI 7 "const_int_operand" "")] ;; mod_f
+  ""
+{
+  enum memmodel model_success = (enum memmodel) INTVAL (operands[6]);
+  enum memmodel model_failure = (enum memmodel) INTVAL (operands[7]);
+  /* Find the union of the two memory models so we can satisfy both success
+     and failure memory models.  */
+  operands[7] = GEN_INT (xr17032_union_memmodel (model_success, model_failure));
+
+  emit_insn (gen_mem_thread_fence (operands[7]));
+  emit_insn (gen_atomic_cas_value_strongsi_internal (operands[1], operands[2],
+						     operands[3], operands[4]));
+  emit_insn (gen_mem_thread_fence (operands[7]));
+
+  rtx compare = operands[1];
+  if (operands[3] != const0_rtx)
+    {
+      rtx difference = gen_rtx_MINUS (SImode, operands[1], operands[3]);
+      compare = gen_reg_rtx (SImode);
+      emit_insn (gen_rtx_SET (compare, difference));
+    }
+
+  emit_insn (gen_rtx_SET (operands[0], gen_rtx_EQ (SImode, compare, const0_rtx)));
+  DONE;
+})
+
+(define_expand "atomic_compare_and_swap<mode>"
+  [(match_operand:SI 0 "register_operand")    ;; bool output
+   (match_operand:QIHI 1 "register_operand") ;; val output
+   (match_operand:QIHI 2 "memory_operand")   ;; memory
+   (match_operand:QIHI 3 "register_operand") ;; expected value
+   (match_operand:QIHI 4 "xr17032_reg_or_0_operand") ;; desired value
+   (match_operand:SI 5 "const_int_operand")   ;; is_weak
+   (match_operand:SI 6 "const_int_operand")   ;; mod_s
+   (match_operand:SI 7 "const_int_operand")]  ;; mod_f
+  ""
+{
+  emit_insn (gen_atomic_cas_value_strong<mode>_internal (operands[1],
+							 operands[2],
+							 operands[3],
+							 operands[4],
+							 operands[6],
+							 operands[7]));
+
+  rtx val = gen_reg_rtx (SImode);
+  if (operands[1] != const0_rtx)
+    emit_move_insn (val, gen_rtx_SIGN_EXTEND (SImode, operands[1]));
+  else
+    emit_move_insn (val, const0_rtx);
+
+  rtx exp = gen_reg_rtx (SImode);
+  if (operands[3] != const0_rtx)
+    emit_move_insn (exp, gen_rtx_SIGN_EXTEND (SImode, operands[3]));
+  else
+    emit_move_insn (exp, const0_rtx);
+
+  rtx compare = val;
+  if (exp != const0_rtx)
+    {
+      rtx difference = gen_rtx_MINUS (SImode, val, exp);
+      compare = gen_reg_rtx (SImode);
+      emit_move_insn (compare, difference);
+    }
+
+  emit_move_insn (operands[0], gen_rtx_EQ (SImode, compare, const0_rtx));
+  DONE;
+})
+
+(define_expand "atomic_cas_value_strong<mode>_internal"
+  [(match_operand:QIHI 0 "register_operand") ;; val output
+   (match_operand:QIHI 1 "memory_operand")   ;; memory
+   (match_operand:QIHI 2 "xr17032_reg_or_0_operand") ;; expected value
+   (match_operand:QIHI 3 "xr17032_reg_or_0_operand") ;; desired value
+   (match_operand:SI 4 "const_int_operand")   ;; mod_s
+   (match_operand:SI 5 "const_int_operand")]  ;; mod_f
+  ""
+{
+  /* We have no QImode/HImode atomics, so form a mask, then use
+     subword_atomic_cas_strong<mode> to implement a LL/SC version of the
+     operation.  */
+
+  rtx old = gen_reg_rtx (SImode);
+  rtx mem = operands[1];
+  rtx aligned_mem = gen_reg_rtx (SImode);
+  rtx shift = gen_reg_rtx (SImode);
+  rtx mask = gen_reg_rtx (SImode);
+  rtx not_mask = gen_reg_rtx (SImode);
+
+  xr17032_subword_address (mem, &aligned_mem, &shift, &mask, &not_mask);
+
+  rtx o = operands[2];
+  rtx n = operands[3];
+  rtx shifted_o = gen_reg_rtx (SImode);
+  rtx shifted_n = gen_reg_rtx (SImode);
+
+  xr17032_lshift_subword (<MODE>mode, o, shift, &shifted_o);
+  xr17032_lshift_subword (<MODE>mode, n, shift, &shifted_n);
+
+  emit_move_insn (shifted_o, gen_rtx_AND (SImode, shifted_o, mask));
+  emit_move_insn (shifted_n, gen_rtx_AND (SImode, shifted_n, mask));
+
+  enum memmodel model_success = (enum memmodel) INTVAL (operands[4]);
+  enum memmodel model_failure = (enum memmodel) INTVAL (operands[5]);
+  /* Find the union of the two memory models so we can satisfy both success
+     and failure memory models.  */
+  rtx model = GEN_INT (xr17032_union_memmodel (model_success, model_failure));
+
+  emit_insn (gen_mem_thread_fence (model));
+  emit_insn (gen_subword_atomic_cas_strong (old, aligned_mem,
+					    shifted_o, shifted_n,
+					    mask, not_mask));
+  emit_insn (gen_mem_thread_fence (model));
+
+  emit_move_insn (old, gen_rtx_ASHIFTRT (SImode, old, shift));
+
+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, old));
+
+  DONE;
+})
+
+(define_insn "subword_atomic_cas_strong"
+  [(set (match_operand:SI 0 "register_operand" "=&r")			   ;; old value at mem
+	(match_operand:SI 1 "memory_operand" "+A"))			   ;; mem location
+   (set (match_dup 1)
+	(unspec_volatile:SI [(match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")  ;; expected value
+			     (match_operand:SI 3 "xr17032_reg_or_0_operand" "rO")] ;; desired value
+	 UNSPEC_COMPARE_AND_SWAP_SUBWORD))
+	(match_operand:SI 4 "register_operand" "rI")			   ;; mask
+	(match_operand:SI 5 "register_operand" "rI")			   ;; not_mask
+	(clobber (match_scratch:SI 6 "=&r"))]				   ;; tmp_1
+  ""
+  {
+    return "1:\;"
+	   "ll\t%0,%A1\;"
+	   "and%i4\t%6,%0,%4\;"
+	   "sub\t%6,%6,%z2\;"
+	   "bne\t%6,1f\;"
+	   "and%i5\t%6,%0,%5\;"
+	   "or\t%6,%6,%z3\;"
+	   "sc\t%6,%A1,%6\;"
+	   "beq\t%6,1b\;"
+	   "1:";
+  }
+  [(set (attr "length") (const_int 32))])
+
+(define_expand "atomic_test_and_set"
+  [(match_operand:QI 0 "register_operand" "")    ;; bool output
+   (match_operand:QI 1 "memory_operand" "+A")    ;; memory
+   (match_operand:SI 2 "const_int_operand" "")]  ;; model
+  ""
+{
+  /* We have no QImode atomics, so use the address LSBs to form a mask,
+     then use an aligned SImode atomic.  */
+  rtx old = gen_reg_rtx (SImode);
+  rtx mem = operands[1];
+  rtx model = operands[2];
+  rtx set = gen_reg_rtx (QImode);
+  rtx aligned_mem = gen_reg_rtx (SImode);
+  rtx shift = gen_reg_rtx (SImode);
+
+  /* Unused.  */
+  rtx _mask = gen_reg_rtx (SImode);
+  rtx _not_mask = gen_reg_rtx (SImode);
+
+  xr17032_subword_address (mem, &aligned_mem, &shift, &_mask, &_not_mask);
+
+  emit_move_insn (set, GEN_INT (1));
+  rtx shifted_set = gen_reg_rtx (SImode);
+  xr17032_lshift_subword (QImode, set, shift, &shifted_set);
+
+  emit_insn (gen_mem_thread_fence (model));
+  emit_insn (gen_atomic_fetch_orsi_internal (old, aligned_mem, shifted_set));
+  emit_insn (gen_mem_thread_fence (model));
+
+  emit_move_insn (old, gen_rtx_ASHIFTRT (SImode, old, shift));
+
+  emit_move_insn (operands[0], gen_lowpart (QImode, old));
+
+  DONE;
+})
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/xr17032-protos.h gcc-workdir/gcc/config/xr17032/xr17032-protos.h
--- gcc-clean/gcc/config/xr17032/xr17032-protos.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/xr17032-protos.h
@@ -0,0 +1,60 @@
+/* Prototypes for xr17032.cc functions used in the md file & elsewhere.
+   Copyright (C) 2025-2025 Free Software Foundation, Inc.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#ifndef GCC_XR17032_PROTOS_H
+#define GCC_XR17032_PROTOS_H
+
+enum xr17032_symbol_type
+{
+  XR17032_SYMBOL_ABSOLUTE,
+  XR17032_SYMBOL_PCREL,
+  XR17032_SYMBOL_GOT_DISP,
+  XR17032_SYMBOL_TLS,
+  XR17032_SYMBOL_TLS_LE,
+  XR17032_SYMBOL_TLS_IE,
+  XR17032_SYMBOL_TLS_GD,
+};
+
+#define XR17032_NUM_SYMBOL_TYPES (XR17032_SYMBOL_TLS_GD + 1)
+
+enum xr17032_epilogue_style
+{
+  XR17032_EPILOGUE_NORMAL,
+  XR17032_EPILOGUE_EH_RETURN,
+};
+
+extern int xr17032_regno_ok_for_base_p (int, bool);
+extern poly_int64 xr17032_initial_elimination_offset (int, int);
+extern void xr17032_expand_prologue (void);
+extern void xr17032_expand_epilogue (enum xr17032_epilogue_style style);
+extern rtx xr17032_return_addr (int);
+extern void xr17032_init_cumulative_args (CUMULATIVE_ARGS *, tree, rtx, tree,
+					  int);
+extern bool xr17032_symbolic_constant_p (rtx, enum xr17032_symbol_type *);
+extern int xr17032_load_store_insns (rtx, rtx_insn *);
+extern void xr17032_move_integer (rtx, rtx, HOST_WIDE_INT);
+extern bool xr17032_split_symbol (rtx, rtx, machine_mode, rtx *);
+extern bool xr17032_split_symbol_type (enum xr17032_symbol_type);
+extern bool xr17032_legitimize_move (machine_mode, rtx, rtx);
+extern const char *xr17032_output_move (rtx, rtx);
+extern enum memmodel xr17032_union_memmodel (enum memmodel a, enum memmodel b);
+extern void xr17032_subword_address (rtx, rtx *, rtx *, rtx *, rtx *);
+extern void xr17032_lshift_subword (machine_mode, rtx, rtx, rtx *);
+
+#endif /* GCC_XR17032_PROTOS_H */
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/xr17032.cc gcc-workdir/gcc/config/xr17032/xr17032.cc
--- gcc-clean/gcc/config/xr17032/xr17032.cc	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/xr17032.cc
@@ -0,0 +1,2355 @@
+/* Target Code for XR/17032
+   Copyright (C) 2025-2025 Free Softwa*re Foundation, Inc.
+   Contributed by monkuous.
+   Based on the RISC-V target.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#define IN_TARGET_CODE 1
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "target.h"
+#include "backend.h"
+#include "tm.h"
+#include "rtl.h"
+#include "regs.h"
+#include "insn-config.h"
+#include "insn-attr.h"
+#include "recog.h"
+#include "output.h"
+#include "alias.h"
+#include "tree.h"
+#include "stringpool.h"
+#include "attribs.h"
+#include "varasm.h"
+#include "stor-layout.h"
+#include "calls.h"
+#include "function.h"
+#include "explow.h"
+#include "ifcvt.h"
+#include "memmodel.h"
+#include "emit-rtl.h"
+#include "reload.h"
+#include "tm_p.h"
+#include "basic-block.h"
+#include "expr.h"
+#include "optabs.h"
+#include "bitmap.h"
+#include "df.h"
+#include "function-abi.h"
+#include "diagnostic.h"
+#include "builtins.h"
+#include "predict.h"
+#include "tree-pass.h"
+#include "opts.h"
+#include "tm-constrs.h"
+#include "rtl-iter.h"
+#include "gimple.h"
+#include "cfghooks.h"
+#include "cfgloop.h"
+#include "cfgrtl.h"
+#include "shrink-wrap.h"
+#include "sel-sched.h"
+#include "sched-int.h"
+#include "fold-const.h"
+#include "gimple-iterator.h"
+#include "gimple-expr.h"
+#include "tree-vectorizer.h"
+#include "gcse.h"
+#include "tree-dfa.h"
+#include "target-globals.h"
+#include "cgraph.h"
+#include "langhooks.h"
+#include "gimplify.h"
+
+/* This file should be included last.  */
+#include "target-def.h"
+
+/* True if X is an UNSPEC wrapper around a SYMBOL_REF or LABEL_REF.  */
+#define UNSPEC_ADDRESS_P(X)					\
+  (GET_CODE (X) == UNSPEC					\
+   && XINT (X, 1) >= UNSPEC_ADDRESS_FIRST			\
+   && XINT (X, 1) < UNSPEC_ADDRESS_FIRST + XR17032_NUM_SYMBOL_TYPES)
+
+/* Extract the symbol type from UNSPEC wrapper X.  */
+#define UNSPEC_ADDRESS_TYPE(X) \
+  ((enum xr17032_symbol_type) (XINT (X, 1) - UNSPEC_ADDRESS_FIRST))
+
+/* Extract the symbol or label from UNSPEC wrapper X.  */
+#define UNSPEC_ADDRESS(X) \
+  XVECEXP (X, 0, 0)
+
+/* Information about a function's frame layout.  */
+struct GTY(())  xr17032_frame_info {
+  /* The size of the frame in bytes.  */
+  poly_int64 total_size;
+
+  /* Bit X is set if the function saves or restores GPR X.  */
+  unsigned int mask;
+
+  /* Offsets of fixed-point save area from frame bottom */
+  poly_int64 gp_sp_offset;
+
+  /* Offset of virtual frame pointer from stack pointer/frame bottom */
+  poly_int64 frame_pointer_offset;
+
+  /* Offset of hard frame pointer from stack pointer/frame bottom */
+  poly_int64 hard_frame_pointer_offset;
+
+  /* The offset of arg_pointer_rtx from the bottom of the frame.  */
+  poly_int64 arg_pointer_offset;
+
+  /* Reset this struct, clean all field to zero.  */
+  void reset(void);
+};
+
+struct GTY(()) machine_function {
+  /* The number of extra stack bytes taken up by register varargs.
+     This area is allocated by the callee at the very top of the frame.  */
+  int varargs_size;
+
+  /* True if current function is a naked function.  */
+  bool naked_p;
+
+  /* True if attributes on current function have been checked.  */
+  bool attributes_checked_p;
+
+  /* True if RA must be saved because of a far jump.  */
+  bool far_jump_used;
+
+  /* The current frame information, calculated by xr17032_compute_frame_info.  */
+  struct xr17032_frame_info frame;
+
+  /* The components already handled by separate shrink-wrapping, which should
+     not be considered by the prologue and epilogue.  */
+  bool reg_is_wrapped_separately[FIRST_PSEUDO_REGISTER];
+};
+
+/* Classifies an address.
+
+   ADDRESS_REG
+       A natural register + offset address.  The register satisfies
+       xr17032_valid_base_register_p and the offset is a const_arith_operand.
+
+   ADDRESS_LO_SUM
+       A LO_SUM rtx.  The first operand is a valid base register and
+       the second operand is a symbolic address.
+
+   ADDRESS_CONST_INT
+       An unsigned 16-bit constant address.
+
+   ADDRESS_SYMBOLIC:
+       A constant symbolic address.  */
+enum xr17032_address_type {
+  ADDRESS_REG,
+  ADDRESS_LO_SUM,
+  ADDRESS_CONST_INT,
+  ADDRESS_SYMBOLIC
+};
+
+/* Information about an address described by xr17032_address_type.
+
+   ADDRESS_CONST_INT
+       No fields are used.
+
+   ADDRESS_REG
+       REG is the base register and OFFSET is the constant offset.
+
+   ADDRESS_LO_SUM
+       REG and OFFSET are the operands to the LO_SUM and SYMBOL_TYPE
+       is the type of symbol it references.
+
+   ADDRESS_SYMBOLIC
+       SYMBOL_TYPE is the type of symbol that the address references.  */
+struct xr17032_address_info {
+  enum xr17032_address_type type;
+  rtx reg;
+  rtx offset;
+  enum xr17032_symbol_type symbol_type;
+  int shift;
+};
+
+/* If non-zero, this is an offset to be added to SP to redefine the CFA
+   when restoring the FP register from the stack.  Only valid when generating
+   the epilogue.  */
+static poly_int64 epilogue_cfa_sp_offset;
+
+/* The __tls_get_attr symbol.  */
+static GTY(()) rtx xr17032_tls_symbol;
+
+void xr17032_frame_info::reset(void)
+{
+  total_size = 0;
+  mask = 0;
+
+  gp_sp_offset = 0;
+
+  frame_pointer_offset = 0;
+
+  hard_frame_pointer_offset = 0;
+
+  arg_pointer_offset = 0;
+}
+
+static struct machine_function *
+xr17032_init_machine_status (void)
+{
+  return ggc_cleared_alloc<machine_function> ();
+}
+
+void
+xr17032_option_override (void)
+{
+  init_machine_status = &xr17032_init_machine_status;
+}
+
+/* Handle stack align for poly_int.  */
+static poly_int64
+xr17032_stack_align (poly_int64 value)
+{
+  return aligned_upper_bound (value, STACK_BOUNDARY / 8);
+}
+
+static HOST_WIDE_INT
+xr17032_stack_align (HOST_WIDE_INT value)
+{
+  return (value + ((STACK_BOUNDARY / 8) - 1)) & ~((STACK_BOUNDARY / 8) - 1);
+}
+
+int
+xr17032_regno_ok_for_base_p (int regno, bool strict_p)
+{
+  if (regno >= FIRST_PSEUDO_REGISTER)
+    {
+      if (!strict_p)
+	return true;
+      regno = reg_renumber[regno];
+    }
+
+  if (regno == ARG_POINTER_REGNUM || regno == FRAME_POINTER_REGNUM)
+    return true;
+
+  return REGNO_REG_CLASS (regno) == GENERAL_REGS;
+}
+
+static bool
+xr17032_far_jump_used_p ()
+{
+  size_t func_size = 0;
+
+  if (cfun->machine->far_jump_used)
+    return true;
+
+  /* We can't change far_jump_used during or after reload, as there is
+     no chance to change stack frame layout.  So we must rely on the
+     conservative heuristic below having done the right thing.  */
+  if (reload_in_progress || reload_completed)
+    return false;
+
+  /* Estimate the function length.  */
+  for (rtx_insn *insn = get_insns (); insn; insn = NEXT_INSN (insn))
+    func_size += get_attr_length (insn);
+
+  /* Conservatively determine whether some jump might exceed 4 MiB
+     displacement.  */
+  if (func_size * 4 >= 0x400000)
+    cfun->machine->far_jump_used = true;
+
+  return cfun->machine->far_jump_used;
+}
+
+static bool
+xr17032_save_return_addr_reg_p (void)
+{
+  /* The lr register is call-clobbered: if this is not a leaf function,
+     save it.  */
+  if (!crtl->is_leaf)
+    return true;
+
+  /* We need to save the incoming return address if __builtin_eh_return
+     is being used to set a different return address.  */
+  if (crtl->calls_eh_return)
+    return true;
+
+  /* Far jumps/branches use lr as a temporary to set up the target jump
+     location (clobbering the incoming return address).  */
+  if (xr17032_far_jump_used_p ())
+    return true;
+
+  /* We need to save it if anyone has used that.  */
+  if (df_regs_ever_live_p (XR17032_LR))
+    return true;
+
+  if (frame_pointer_needed && crtl->is_leaf)
+    return true;
+
+  return false;
+}
+
+static bool
+xr17032_save_reg_p (unsigned int regno)
+{
+  bool call_saved = !global_regs[regno] && !call_used_or_fixed_reg_p (regno);
+  bool might_clobber = crtl->saves_all_registers
+		       || df_regs_ever_live_p (regno);
+
+  if (call_saved && might_clobber)
+    return true;
+
+  if (regno == HARD_FRAME_POINTER_REGNUM && frame_pointer_needed)
+    return true;
+
+  if (regno == XR17032_LR && xr17032_save_return_addr_reg_p ())
+    return true;
+
+  return false;
+}
+
+static void
+xr17032_compute_frame_info (void)
+{
+  struct xr17032_frame_info *frame;
+  poly_int64 offset;
+  unsigned int regno, i, num_x_saved = 0, x_save_size = 0;
+
+  frame = &cfun->machine->frame;
+
+  /* Adjust the outgoing arguments size if required.  Keep it in sync with what
+     the mid-end is doing.  */
+  crtl->outgoing_args_size = STACK_DYNAMIC_OFFSET (cfun);
+
+  frame->reset();
+
+  if (!cfun->machine->naked_p)
+    {
+      /* Find out which GPRs we need to save.  */
+      for (regno = 0; regno <= 31; regno++)
+	if (xr17032_save_reg_p (regno))
+	  frame->mask |= 1 << regno, num_x_saved++;
+
+      /* If this function calls eh_return, we must also save and restore the
+	 EH data registers.  */
+      if (crtl->calls_eh_return)
+	for (i = 0; (regno = EH_RETURN_DATA_REGNO (i)) != INVALID_REGNUM; i++)
+	  frame->mask |= 1 << regno, num_x_saved++;
+    }
+
+  if (frame->mask)
+    {
+      x_save_size = xr17032_stack_align (num_x_saved * UNITS_PER_WORD);
+    }
+
+  /* At the bottom of the frame are any outgoing stack arguments. */
+  offset = xr17032_stack_align (crtl->outgoing_args_size);
+  /* The hard frame pointer points to the local variables. */
+  frame->hard_frame_pointer_offset = offset;
+  /* Next are local stack variables. */
+  offset += xr17032_stack_align (get_frame_size ());
+  /* The virtual frame pointer points to the callee-saved GPRs. */
+  frame->frame_pointer_offset = offset;
+  /* Next are the callee-saved GPRs. */
+  if (frame->mask)
+    {
+      offset += x_save_size;
+    }
+  frame->gp_sp_offset = offset - UNITS_PER_WORD;
+  /* Above the saved GPRs is the callee-allocated varags save area. */
+  offset += xr17032_stack_align (cfun->machine->varargs_size);
+  /* Next is the callee-allocated area for pretend stack arguments.  */
+  offset += xr17032_stack_align (crtl->args.pretend_args_size);
+  /* Arg pointer must be below pretend args, but must be above alignment
+     padding.  */
+  frame->arg_pointer_offset = offset - crtl->args.pretend_args_size;
+  frame->total_size = offset;
+
+  /* Next points the incoming stack pointer and any incoming arguments. */
+}
+
+poly_int64
+xr17032_initial_elimination_offset (int from, int to)
+{
+  poly_int64 src, dest;
+
+  xr17032_compute_frame_info ();
+
+  if (to == HARD_FRAME_POINTER_REGNUM)
+    dest = cfun->machine->frame.hard_frame_pointer_offset;
+  else if (to == STACK_POINTER_REGNUM)
+    dest = 0; /* The stack pointer is the base of all offsets, hence 0.  */
+  else
+    gcc_unreachable ();
+
+  if (from == FRAME_POINTER_REGNUM)
+    src = cfun->machine->frame.frame_pointer_offset;
+  else if (from == ARG_POINTER_REGNUM)
+    src = cfun->machine->frame.arg_pointer_offset;
+  else
+    gcc_unreachable ();
+
+  return src - dest;
+}
+
+static bool
+xr17032_valid_base_register_p (rtx x, bool strict_p)
+{
+  if (!strict_p && SUBREG_P (x))
+    x = SUBREG_REG (x);
+
+  return (REG_P (x) && xr17032_regno_ok_for_base_p (REGNO (x), strict_p));
+}
+
+static bool
+xr17032_valid_offset_p (rtx x, machine_mode mode)
+{
+  /* Check that X is an unsigned 16-bit number.  */
+  if (!xr17032_smallimm_operand (x, Pmode))
+    return false;
+
+  /* We may need to split multiword moves, so make sure that every word
+     is accessible.  */
+  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD
+      && !SMALL_OPERAND (INTVAL (x) + GET_MODE_SIZE (mode) - UNITS_PER_WORD))
+    return false;
+
+  return true;
+}
+
+/* For stack frames that can't be allocated with a single ADDI instruction,
+   compute the best value to initially allocate.  It must at a minimum
+   allocate enough space to spill the callee-saved registers.  */
+
+static HOST_WIDE_INT
+xr17032_first_stack_step (struct xr17032_frame_info *frame,
+			  poly_int64 remaining_size)
+{
+  HOST_WIDE_INT remaining_const_size;
+  if (!remaining_size.is_constant ())
+    remaining_const_size
+      = xr17032_stack_align (remaining_size.coeffs[0])
+	- xr17032_stack_align (remaining_size.coeffs[1]);
+  else
+    remaining_const_size = remaining_size.to_constant ();
+
+  if (SMALL_OPERAND (remaining_const_size))
+    return remaining_const_size;
+
+  poly_int64 callee_saved_first_step =
+    remaining_size - frame->frame_pointer_offset;
+  gcc_assert(callee_saved_first_step.is_constant ());
+  HOST_WIDE_INT min_first_step =
+    xr17032_stack_align (callee_saved_first_step.to_constant ());
+  HOST_WIDE_INT max_first_step = IMM_REACH - STACK_BOUNDARY / 8;
+  HOST_WIDE_INT min_second_step = remaining_const_size - max_first_step;
+  gcc_assert (min_first_step <= max_first_step);
+
+  /* As an optimization, use the least-significant bits of the total frame
+     size, so that the second adjustment step is just LUI + ADD.  */
+  if (!SMALL_OPERAND (min_second_step)
+      && remaining_const_size % IMM_REACH <= max_first_step
+      && remaining_const_size % IMM_REACH >= min_first_step)
+    return remaining_const_size % IMM_REACH;
+
+  return max_first_step;
+}
+
+/* A function to save or store a register.  The first argument is the
+   register and the second is the stack slot.  */
+typedef void (*xr17032_save_restore_fn) (rtx, rtx);
+
+/* Use FN to save or restore register REGNO.  MODE is the register's
+   mode and OFFSET is the offset of its save slot from the current
+   stack pointer.  */
+
+static void
+xr17032_save_restore_reg (machine_mode mode, int regno,
+			  HOST_WIDE_INT offset, xr17032_save_restore_fn fn)
+{
+  rtx mem;
+
+  mem = gen_frame_mem (mode, plus_constant (Pmode, stack_pointer_rtx, offset));
+  fn (gen_rtx_REG (mode, regno), mem);
+}
+
+/* Return the next register up from REGNO up to LIMIT for the callee
+ *  to save or restore.  OFFSET will be adjusted accordingly.
+ *  If INC is set, then REGNO will be incremented first.
+ *  Returns INVALID_REGNUM if there is no such next register.  */
+
+static unsigned int
+xr17032_next_saved_reg (unsigned int regno, unsigned int limit,
+			HOST_WIDE_INT *offset, bool inc = true)
+{
+  if (inc)
+    regno++;
+
+  while (regno <= limit)
+  {
+    if (cfun->machine->frame.mask & (1U << regno))
+    {
+      *offset = *offset - UNITS_PER_WORD;
+      return regno;
+    }
+
+    regno++;
+  }
+  return INVALID_REGNUM;
+}
+
+static bool
+xr17032_is_eh_return_data_register (unsigned int regno)
+{
+  unsigned int i, regnum;
+
+  if (!crtl->calls_eh_return)
+    return false;
+
+  for (i = 0; (regnum = EH_RETURN_DATA_REGNO (i)) != INVALID_REGNUM; i++)
+    if (regno == regnum)
+      {
+	return true;
+      }
+
+  return false;
+}
+
+/* Call FN for each register that is saved by the current function.
+   SP_OFFSET is the offset of the current stack pointer from the start
+   of the frame.  */
+
+static void
+xr17032_for_each_saved_reg (poly_int64 sp_offset, xr17032_save_restore_fn fn,
+			    bool epilogue, bool maybe_eh_return)
+{
+  HOST_WIDE_INT offset;
+  unsigned int regno;
+  unsigned int start = 0;
+  unsigned int limit = 31;
+
+  /* Save the link register and s-registers. */
+  offset = (cfun->machine->frame.gp_sp_offset - sp_offset).to_constant ()
+	   + UNITS_PER_WORD;
+  for (regno = xr17032_next_saved_reg (start, limit, &offset, false);
+       regno != INVALID_REGNUM;
+       regno = xr17032_next_saved_reg (regno, limit, &offset))
+    {
+      if (cfun->machine->reg_is_wrapped_separately[regno])
+	continue;
+
+      if (epilogue && !maybe_eh_return
+	  && xr17032_is_eh_return_data_register (regno))
+	continue;
+
+      xr17032_save_restore_reg (word_mode, regno, offset, fn);
+    }
+}
+
+static void
+xr17032_emit_stack_tie (rtx reg)
+{
+  emit_insn (gen_xr17032_stack_tie (stack_pointer_rtx, reg));
+}
+
+/* Constant VAL is known to be sum of two U16 constants.  Break it into
+   comprising BASE and OFF.
+   Numerically U16 is 0 to 65535, however it uses the more conservative
+   range 0 to 65528 as offsets pertain to stack related registers.  */
+
+void
+xr17032_split_sum_of_two_u16 (HOST_WIDE_INT val, HOST_WIDE_INT *base,
+			      HOST_WIDE_INT *off)
+{
+  if (SUM_OF_TWO_U16_ALGN (val))
+    {
+      *base = IMM_REACH & ~3;
+      *off = val - *base;
+    }
+  else
+    {
+      gcc_unreachable ();
+    }
+}
+
+/* Emit a move from SRC to DEST.  Assume that the move expanders can
+   handle all moves if !can_create_pseudo_p ().  The distinction is
+   important because, unlike emit_move_insn, the move expanders know
+   how to force Pmode objects into the constant pool even when the
+   constant pool address is not itself legitimate.  */
+
+rtx
+xr17032_emit_move (rtx dest, rtx src)
+{
+  return (can_create_pseudo_p ()
+	  ? emit_move_insn (dest, src)
+	  : emit_move_insn_1 (dest, src));
+}
+
+/* Make the last instruction frame-related and note that it performs
+   the operation described by FRAME_PATTERN.  */
+
+static void
+xr17032_set_frame_expr (rtx frame_pattern)
+{
+  rtx insn;
+
+  insn = get_last_insn ();
+  RTX_FRAME_RELATED_P (insn) = 1;
+  REG_NOTES (insn) = alloc_EXPR_LIST (REG_FRAME_RELATED_EXPR,
+				      frame_pattern,
+				      REG_NOTES (insn));
+}
+
+/* Return a frame-related rtx that stores REG at MEM.
+   REG must be a single register.  */
+
+static rtx
+xr17032_frame_set (rtx mem, rtx reg)
+{
+  rtx set = gen_rtx_SET (mem, reg);
+  RTX_FRAME_RELATED_P (set) = 1;
+  return set;
+}
+
+/* Save register REG to MEM.  Make the instruction frame-related.  */
+
+static void
+xr17032_save_reg (rtx reg, rtx mem)
+{
+  xr17032_emit_move (mem, reg);
+  xr17032_set_frame_expr (xr17032_frame_set (mem, reg));
+}
+
+/* Copy VALUE to a register and return that register.  If new pseudos
+   are allowed, copy it into a new register, otherwise use DEST.  */
+
+static rtx
+xr17032_force_temporary (rtx dest, rtx value)
+{
+  if (can_create_pseudo_p ())
+    return force_reg (Pmode, value);
+  else
+    {
+      xr17032_emit_move (dest, value);
+      return dest;
+    }
+}
+
+/* Allocate SIZE bytes of stack space using TEMP1 as a scratch register.  */
+
+static void
+xr17032_allocate_and_probe_stack_space (rtx temp1, HOST_WIDE_INT size)
+{
+  rtx insn;
+
+  if (SMALL_OPERAND (size))
+    {
+      insn = gen_sub3_insn (stack_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT (size));
+      RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+    }
+  else if (SUM_OF_TWO_U16_ALGN (size))
+    {
+      HOST_WIDE_INT one, two;
+      xr17032_split_sum_of_two_u16 (size, &one, &two);
+      insn = gen_sub3_insn (stack_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT (one));
+      RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+      insn = gen_sub3_insn (stack_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT (two));
+      RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+    }
+  else
+    {
+      temp1 = xr17032_force_temporary (temp1, GEN_INT (size));
+      emit_insn (gen_sub3_insn (stack_pointer_rtx, stack_pointer_rtx, temp1));
+      insn = plus_constant (Pmode, stack_pointer_rtx, size);
+      insn = gen_rtx_SET (stack_pointer_rtx, insn);
+      xr17032_set_frame_expr (insn);
+    }
+
+  /* We must have allocated the remainder of the stack frame.
+     Emit a stack tie if we have a frame pointer so that the
+     allocation is ordered WRT fp setup and subsequent writes
+     into the frame.  */
+  if (frame_pointer_needed)
+    xr17032_emit_stack_tie (hard_frame_pointer_rtx);
+}
+
+void
+xr17032_expand_prologue (void)
+{
+  struct xr17032_frame_info *frame = &cfun->machine->frame;
+  poly_int64 remaining_size = frame->total_size;
+  rtx insn;
+
+  if (cfun->machine->naked_p)
+    return;
+
+  /* Save the GP registers */
+  if (frame->mask != 0)
+    {
+      if (known_gt (remaining_size, frame->frame_pointer_offset))
+	{
+	  HOST_WIDE_INT step1 = xr17032_first_stack_step (frame,
+							  remaining_size);
+	  remaining_size -= step1;
+	  insn = gen_sub3_insn (stack_pointer_rtx, stack_pointer_rtx,
+				GEN_INT (step1));
+	  RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+	}
+      xr17032_for_each_saved_reg (remaining_size, xr17032_save_reg,
+				  false, false);
+    }
+
+  /* Set up the frame pointer, if we're using one.  */
+  if (frame_pointer_needed)
+    {
+      insn = gen_add3_insn (hard_frame_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT ((frame->hard_frame_pointer_offset - remaining_size).to_constant ()));
+      RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+
+      xr17032_emit_stack_tie (hard_frame_pointer_rtx);
+    }
+
+  /* Allocate the rest of the frame.  */
+  if (known_gt (remaining_size, 0))
+    {
+      gcc_assert (remaining_size.is_constant ());
+      xr17032_allocate_and_probe_stack_space (XR17032_PROLOGUE_TEMP (Pmode),
+					      remaining_size.to_constant ());
+    }
+}
+
+bool
+xr17032_can_use_return_insn (void)
+{
+  return reload_completed && known_eq (cfun->machine->frame.total_size, 0);
+}
+
+static void
+xr17032_restore_reg (rtx reg, rtx mem)
+{
+  rtx insn = xr17032_emit_move (reg, mem);
+  xr17032_set_frame_expr (xr17032_frame_set (reg, mem));
+  rtx dwarf = NULL_RTX;
+  dwarf = alloc_reg_note (REG_CFA_RESTORE, reg, dwarf);
+
+  if (known_gt (epilogue_cfa_sp_offset, 0)
+      && REGNO (reg) == HARD_FRAME_POINTER_REGNUM)
+    {
+      rtx cfa_adjust_rtx
+	= gen_rtx_PLUS (Pmode, stack_pointer_rtx,
+			gen_int_mode (epilogue_cfa_sp_offset, Pmode));
+      dwarf = alloc_reg_note (REG_CFA_DEF_CFA, cfa_adjust_rtx, dwarf);
+    }
+
+  REG_NOTES (insn) = dwarf;
+  RTX_FRAME_RELATED_P (insn) = 1;
+}
+
+void
+xr17032_expand_epilogue (enum xr17032_epilogue_style style)
+{
+  /* Split the frame into 2 steps. STEP1 is the amount of stack we should
+     deallocate before restoring the registers. STEP2 is the amount we
+     should deallocate afterwards including the callee saved regs.
+
+     Start off by assuming that no registers need to be restored.  */
+  struct xr17032_frame_info *frame = &cfun->machine->frame;
+  poly_int64 step2 = 0;
+
+  rtx insn;
+
+  /* We need to add memory barrier to prevent read from deallocated stack.  */
+  bool need_barrier_p = known_ne (get_frame_size ()
+				  + cfun->machine->frame.arg_pointer_offset, 0);
+
+  if (cfun->machine->naked_p)
+    return;
+
+  if (style == XR17032_EPILOGUE_NORMAL && xr17032_can_use_return_insn ())
+    {
+      emit_jump_insn (gen_xr17032_return ());
+      return;
+    }
+
+  /* Reset the epilogue cfa info before starting to emit the epilogue.  */
+  epilogue_cfa_sp_offset = 0;
+
+  /* Move past any dynamic stack allocations.  */
+  if (cfun->calls_alloca)
+    {
+      /* Emit a barrier to prevent loads from a deallocated stack.  */
+      xr17032_emit_stack_tie (hard_frame_pointer_rtx);
+      need_barrier_p = false;
+
+      poly_int64 adjust_offset = -frame->hard_frame_pointer_offset;
+      rtx dwarf_adj = gen_int_mode (adjust_offset, Pmode);
+      rtx adjust = NULL_RTX;
+      bool sum_of_two_u16 = false;
+      HOST_WIDE_INT one, two;
+
+      gcc_assert (adjust_offset.is_constant ());
+
+      HOST_WIDE_INT adj_off_value = adjust_offset.to_constant ();
+      if (SMALL_OPERAND (adj_off_value))
+	{
+	  adjust = GEN_INT (adj_off_value);
+	}
+      else if (SUM_OF_TWO_U16_ALGN (adj_off_value))
+	{
+	  xr17032_split_sum_of_two_u16 (adj_off_value, &one, &two);
+	  dwarf_adj = adjust = GEN_INT (one);
+	  sum_of_two_u16 = true;
+	}
+      else
+	{
+	  xr17032_emit_move (XR17032_PROLOGUE_TEMP (Pmode),
+			     GEN_INT (adj_off_value));
+	  adjust = XR17032_PROLOGUE_TEMP (Pmode);
+	}
+
+      insn = emit_insn (
+	       gen_add3_insn (stack_pointer_rtx, hard_frame_pointer_rtx,
+			      adjust));
+
+      rtx dwarf = NULL_RTX;
+      rtx cfa_adjust_value = gen_rtx_PLUS (Pmode, hard_frame_pointer_rtx,
+					   dwarf_adj);
+      rtx cfa_adjust_rtx = gen_rtx_SET (stack_pointer_rtx, cfa_adjust_value);
+      dwarf = alloc_reg_note (REG_CFA_ADJUST_CFA, cfa_adjust_rtx, dwarf);
+
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      REG_NOTES (insn) = dwarf;
+
+      if (sum_of_two_u16)
+	{
+	  insn = emit_insn (gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT (two)));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+    }
+
+  if (frame->mask != 0)
+    step2 = xr17032_first_stack_step (frame, frame->total_size);
+
+  poly_int64 step1 = frame->total_size - step2;
+
+  /* Set TARGET to BASE + STEP1.  */
+  if (known_gt (step1, 0))
+    {
+      /* Emit a barrier to prevent loads from a deallocated stack.  */
+      xr17032_emit_stack_tie (hard_frame_pointer_rtx);
+      need_barrier_p = false;
+
+      gcc_assert (step1.is_constant ());
+
+      /* Get an rtx for STEP1 that we can add to BASE.
+	 Skip if adjust equal to zero.  */
+      HOST_WIDE_INT step1_value = step1.to_constant ();
+      if (step1_value != 0)
+	{
+	  rtx adjust = GEN_INT (step1_value);
+	  if (SUM_OF_TWO_U16_ALGN (step1_value))
+	    {
+	      HOST_WIDE_INT one, two;
+	      xr17032_split_sum_of_two_u16 (step1_value, &one, &two);
+	      insn = emit_insn (gen_add3_insn (stack_pointer_rtx,
+						stack_pointer_rtx,
+						GEN_INT (one)));
+	      RTX_FRAME_RELATED_P (insn) = 1;
+	      adjust = GEN_INT (two);
+	    }
+	  else if (!SMALL_OPERAND (step1_value))
+	    {
+	      xr17032_emit_move (XR17032_PROLOGUE_TEMP (Pmode), adjust);
+	      adjust = XR17032_PROLOGUE_TEMP (Pmode);
+	    }
+
+	  insn = emit_insn (gen_add3_insn (stack_pointer_rtx,
+					   stack_pointer_rtx,
+					   adjust));
+	  rtx dwarf = NULL_RTX;
+	  rtx cfa_adjust_rtx
+	    = gen_rtx_PLUS (Pmode, stack_pointer_rtx,
+			    gen_int_mode (step2, Pmode));
+
+	  dwarf = alloc_reg_note (REG_CFA_DEF_CFA, cfa_adjust_rtx, dwarf);
+	  RTX_FRAME_RELATED_P (insn) = 1;
+
+	  REG_NOTES (insn) = dwarf;
+	}
+    }
+  else if (frame_pointer_needed)
+    {
+      /* Tell xr17032_restore_reg to emit dwarf to redefine CFA when restoring
+	 old value of FP.  */
+      epilogue_cfa_sp_offset = step2;
+    }
+
+  /* Restore the registers.  */
+  xr17032_for_each_saved_reg (frame->total_size - step2, xr17032_restore_reg,
+			      true, style == XR17032_EPILOGUE_EH_RETURN);
+
+  if (need_barrier_p)
+    xr17032_emit_stack_tie (hard_frame_pointer_rtx);
+
+  /* Deallocate the final bit of the frame.  */
+  if (step2.to_constant () > 0)
+    {
+      insn = emit_insn (gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
+				       GEN_INT (step2.to_constant ())));
+
+      rtx dwarf = NULL_RTX;
+      rtx cfa_adjust_rtx
+	= gen_rtx_PLUS (Pmode, stack_pointer_rtx, GEN_INT (0));
+      dwarf = alloc_reg_note (REG_CFA_DEF_CFA, cfa_adjust_rtx, dwarf);
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      REG_NOTES (insn) = dwarf;
+    }
+
+  emit_jump_insn (gen_xr17032_return ());
+}
+
+static bool
+xr17032_frame_pointer_required (void)
+{
+  return frame_pointer_needed;
+}
+
+rtx
+xr17032_return_addr (int count)
+{
+  return count ? const0_rtx : get_hard_reg_initial_val (Pmode, XR17032_LR);
+}
+
+struct xr17032_arg_info
+{
+  /* True if the argument is at least partially passed on the stack.  */
+  bool stack_p;
+
+  /* The number of integer registers allocated to this argument.  */
+  unsigned int num_gprs;
+
+  /* The offset of the first register used, provided num_gprs is nonzero.
+     If passed entirely on the stack, the value is MAX_ARGS_IN_REGISTERS.  */
+  unsigned int gpr_offset;
+};
+
+/* Implement TARGET_FUNCTION_ARG_BOUNDARY.  Every parameter gets at
+   least PARM_BOUNDARY bits of alignment, but will be given anything up
+   to PREFERRED_STACK_BOUNDARY bits if the type requires it.  */
+
+static unsigned int
+xr17032_function_arg_boundary (machine_mode mode, const_tree type)
+{
+  unsigned int alignment;
+
+  /* Use natural alignment if the type is not aggregate data.  */
+  if (type && !AGGREGATE_TYPE_P (type))
+    alignment = TYPE_ALIGN (TYPE_MAIN_VARIANT (type));
+  else
+    alignment = type ? TYPE_ALIGN (type) : GET_MODE_ALIGNMENT (mode);
+
+  return MIN (STACK_BOUNDARY, MAX (PARM_BOUNDARY, alignment));
+}
+
+typedef struct {
+  const_tree type;
+  HOST_WIDE_INT offset;
+} xr17032_aggregate_field;
+
+/* Fill INFO with information about a single argument, and return an RTL
+   pattern to pass or return the argument. Return NULL_RTX if argument cannot
+   pass or return in registers, then the argument may be passed by reference or
+   through the stack or  .  CUM is the cumulative state for earlier arguments.
+   MODE is the mode of this argument and TYPE is its type (if known). NAMED is
+   true if this is a named (fixed) argument rather than a variable one. RETURN_P
+   is true if returning the argument, or false if passing the argument.  */
+
+static rtx
+xr17032_get_arg_info (struct xr17032_arg_info *info, const CUMULATIVE_ARGS *cum,
+		      machine_mode mode, const_tree type, bool named,
+		      bool return_p)
+{
+  unsigned num_bytes, num_words;
+  unsigned gpr_base = return_p ? XR17032_A3 : XR17032_A0;
+
+  memset (info, 0, sizeof (*info));
+  info->gpr_offset = cum->num_gprs;
+
+  /* Work out the size of the argument.  */
+  num_bytes = type ? int_size_in_bytes (type) : GET_MODE_SIZE (mode);
+  num_words = (num_bytes + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+
+  if (!named || (!return_p && num_words > 2))
+    info->stack_p = true;
+  else
+    info->stack_p = num_words > MAX_ARGS_IN_REGISTERS - info->gpr_offset;
+
+  if (info->stack_p)
+    {
+      info->num_gprs = 0;
+      return NULL_RTX;
+    }
+
+  info->num_gprs = num_words;
+
+  if (return_p)
+    {
+      if (num_words <= 1)
+	return gen_rtx_REG (mode, gpr_base - info->gpr_offset);
+
+      rtvec vec = rtvec_alloc (num_words);
+
+      for (unsigned i = 0; i < num_words; i++)
+	{
+	  rtx reg = gen_rtx_REG (SImode, gpr_base - info->gpr_offset - i);
+	  rtx off = gen_rtx_CONST_INT (SImode, i * UNITS_PER_WORD);
+	  RTVEC_ELT (vec, i) = gen_rtx_EXPR_LIST (SImode, reg, off);
+	}
+
+      return gen_rtx_PARALLEL (mode, vec);
+    }
+
+  return gen_rtx_REG (mode, gpr_base + info->gpr_offset);
+}
+
+static rtx
+xr17032_function_arg (cumulative_args_t cum_v, const function_arg_info &arg)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+  struct xr17032_arg_info info;
+
+  if (arg.end_marker_p ())
+    return const0_rtx;
+
+  return xr17032_get_arg_info (&info, cum, arg.mode, arg.type, arg.named,
+			       false);
+}
+
+static void
+xr17032_function_arg_advance (cumulative_args_t cum_v,
+			      const function_arg_info &arg)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+  struct xr17032_arg_info info;
+
+  xr17032_get_arg_info (&info, cum, arg.mode, arg.type, arg.named, false);
+
+  cum->num_gprs = info.gpr_offset + info.num_gprs;
+}
+
+/* Initialize a variable CUM of type CUMULATIVE_ARGS
+   for a call to a function whose data type is FNTYPE.
+   For a library call, FNTYPE is 0.  */
+
+void
+xr17032_init_cumulative_args (CUMULATIVE_ARGS *cum, tree fntype, rtx, tree, int)
+{
+  memset (cum, 0, sizeof (*cum));
+}
+
+static rtx
+xr17032_function_value (const_tree type, const_tree func, bool)
+{
+  CUMULATIVE_ARGS cum;
+  struct xr17032_arg_info info;
+  machine_mode mode = TYPE_MODE (type);
+
+  memset (&cum, 0, sizeof (cum));
+
+  int unsigned_p = TYPE_UNSIGNED (type);
+  mode = promote_function_mode (type, mode, &unsigned_p, func, 1);
+
+  return xr17032_get_arg_info (&info, &cum, mode, type, true, true);
+}
+
+static rtx
+xr17032_libcall_value (machine_mode mode, const_rtx)
+{
+  CUMULATIVE_ARGS cum;
+  struct xr17032_arg_info info;
+
+  memset (&cum, 0, sizeof (cum));
+
+  return xr17032_get_arg_info (&info, &cum, mode, nullptr, true, true);
+}
+
+static bool
+xr17032_function_value_regno_p (const unsigned int regno)
+{
+  return regno >= XR17032_A0 && regno <= XR17032_A3;
+}
+
+static bool
+xr17032_return_in_memory (const_tree type, const_tree func)
+{
+  CUMULATIVE_ARGS cum;
+  struct xr17032_arg_info info;
+  machine_mode mode = TYPE_MODE (type);
+
+  memset (&cum, 0, sizeof (cum));
+
+  int unsigned_p = TYPE_UNSIGNED (type);
+  mode = promote_function_mode (type, mode, &unsigned_p, func, 1);
+
+  xr17032_get_arg_info (&info, &cum, mode, type, true, true);
+
+  return info.stack_p;
+}
+
+/* Return true if X is a thread-local symbol.  */
+
+static bool
+xr17032_tls_symbol_p (const_rtx x)
+{
+  return SYMBOL_REF_P (x) && SYMBOL_REF_TLS_MODEL (x) != 0;
+}
+
+/* Return true if symbol X binds locally.  */
+
+static bool
+xr17032_symbol_binds_local_p (const_rtx x)
+{
+  if (SYMBOL_REF_P (x))
+    return (SYMBOL_REF_DECL (x)
+	    ? targetm.binds_local_p (SYMBOL_REF_DECL (x))
+	    : SYMBOL_REF_LOCAL_P (x));
+  else
+    return false;
+}
+
+/* Return the method that should be used to access SYMBOL_REF or
+   LABEL_REF X.  */
+
+static enum xr17032_symbol_type
+xr17032_classify_symbol (const_rtx x)
+{
+  if (xr17032_tls_symbol_p (x))
+    return XR17032_SYMBOL_TLS;
+
+  if (GET_CODE (x) == SYMBOL_REF && flag_pic && !xr17032_symbol_binds_local_p (x))
+    return XR17032_SYMBOL_GOT_DISP;
+
+  return flag_pic ? XR17032_SYMBOL_PCREL : XR17032_SYMBOL_ABSOLUTE;
+}
+
+/* Return true if X is a symbolic constant.  If it is, store the type of
+   the symbol in *SYMBOL_TYPE.  */
+
+bool
+xr17032_symbolic_constant_p (rtx x, enum xr17032_symbol_type *symbol_type)
+{
+  rtx offset;
+
+  split_const (x, &x, &offset);
+  if (UNSPEC_ADDRESS_P (x))
+    {
+      *symbol_type = UNSPEC_ADDRESS_TYPE (x);
+      x = UNSPEC_ADDRESS (x);
+    }
+  else if (GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF)
+    *symbol_type = xr17032_classify_symbol (x);
+  else
+    return false;
+
+  if (offset == const0_rtx)
+    return true;
+
+  /* Nonzero offsets are only valid for references that don't use the GOT.  */
+  switch (*symbol_type)
+    {
+    case XR17032_SYMBOL_ABSOLUTE:
+    case XR17032_SYMBOL_PCREL:
+    case XR17032_SYMBOL_TLS_LE:
+      /* GAS rejects offsets outside the range [-2^31, 2^31-1].  */
+      return sext_hwi (INTVAL (offset), 32) == INTVAL (offset);
+
+    default:
+      return false;
+    }
+}
+
+static void
+xr17032_expand_builtin_va_start (tree valist, rtx nextarg)
+{
+  nextarg = gen_rtx_MINUS (Pmode, nextarg, GEN_INT (cfun->machine->varargs_size));
+  std_expand_builtin_va_start (valist, nextarg);
+}
+
+/* Implement TARGET_CANNOT_COPY_INSN_P.  */
+
+static bool
+xr17032_cannot_copy_insn_p (rtx_insn *insn)
+{
+  return recog_memoized (insn) >= 0 && get_attr_cannot_copy (insn);
+}
+
+/* Load an entry from the GOT for a TLS GD access.  */
+
+static rtx xr17032_got_load_tls_gd (rtx dest, rtx sym)
+{
+  return gen_xr17032_got_load_tls_gd (dest, sym);
+}
+
+/* Return an instruction sequence that calls __tls_get_addr.  SYM is
+   the TLS symbol we are referencing and TYPE is the symbol type to use
+   (either global dynamic or local dynamic).  RESULT is an RTX for the
+   return value location.  */
+
+static rtx_insn *
+xr17032_call_tls_get_addr (rtx sym, rtx result)
+{
+  rtx a0 = gen_rtx_REG (Pmode, XR17032_A0), func;
+  rtx_insn *insn;
+
+  if (!xr17032_tls_symbol)
+    xr17032_tls_symbol = init_one_libfunc ("__tls_get_addr");
+  func = gen_rtx_MEM (FUNCTION_MODE, xr17032_tls_symbol);
+
+  start_sequence ();
+
+  emit_insn (xr17032_got_load_tls_gd (a0, sym));
+  insn = emit_call_insn (gen_call_value (result, func, const0_rtx));
+  RTL_CONST_CALL_P (insn) = 1;
+  use_reg (&CALL_INSN_FUNCTION_USAGE (insn), a0);
+  insn = get_insns ();
+
+  end_sequence ();
+
+  return insn;
+}
+
+/* Load an entry from the GOT for a TLS IE access.  */
+
+static rtx xr17032_got_load_tls_ie (rtx dest, rtx sym)
+{
+  return gen_xr17032_got_load_tls_ie (dest, sym);
+}
+
+/* Wrap symbol or label BASE in an UNSPEC address of type SYMBOL_TYPE,
+   then add CONST_INT OFFSET to the result.  */
+
+static rtx
+xr17032_unspec_address_offset (rtx base, rtx offset,
+			       enum xr17032_symbol_type symbol_type)
+{
+  base = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, base),
+			 UNSPEC_ADDRESS_FIRST + symbol_type);
+  if (offset != const0_rtx)
+    base = gen_rtx_PLUS (Pmode, base, offset);
+  return gen_rtx_CONST (Pmode, base);
+}
+
+/* Return an UNSPEC address with underlying address ADDRESS and symbol
+   type SYMBOL_TYPE.  */
+
+rtx
+xr17032_unspec_address (rtx address, enum xr17032_symbol_type symbol_type)
+{
+  rtx base, offset;
+
+  split_const (address, &base, &offset);
+  return xr17032_unspec_address_offset (base, offset, symbol_type);
+}
+
+/* If xr17032_unspec_address (ADDR, SYMBOL_TYPE) is a 32-bit value, add the
+   high part to BASE and return the result.  Just return BASE otherwise.
+   TEMP is as for xr17032_force_temporary.
+
+   The returned expression can be used as the first operand to a LO_SUM.  */
+
+static rtx
+xr17032_unspec_offset_high (rtx temp, rtx addr, enum xr17032_symbol_type symbol_type)
+{
+  addr = gen_rtx_HIGH (Pmode, xr17032_unspec_address (addr, symbol_type));
+  return xr17032_force_temporary (temp, addr);
+}
+
+/* Add in the thread pointer for a TLS LE access.  */
+
+static rtx xr17032_tls_add_tp_le (rtx dest, rtx base, rtx sym)
+{
+  rtx tp = gen_rtx_REG (Pmode, XR17032_TP);
+  return gen_xr17032_tls_add_tp_le (dest, base, tp, sym);
+}
+
+/* Generate the code to access LOC, a thread-local SYMBOL_REF, and return
+   its address.  The return value will be both a valid address and a valid
+   SET_SRC (either a REG or a LO_SUM).  */
+
+static rtx
+xr17032_legitimize_tls_address (rtx loc)
+{
+  rtx dest, tp, tmp;
+  enum tls_model model = SYMBOL_REF_TLS_MODEL (loc);
+
+  switch (model)
+    {
+    case TLS_MODEL_LOCAL_DYNAMIC:
+      /* Rely on section anchors for the optimization that LDM TLS
+	 provides.  The anchor's address is loaded with GD TLS. */
+    case TLS_MODEL_GLOBAL_DYNAMIC:
+      tmp = gen_rtx_REG (Pmode, XR17032_A3);
+      dest = gen_reg_rtx (Pmode);
+      emit_libcall_block (xr17032_call_tls_get_addr (loc, tmp), dest, tmp,
+			  loc);
+      break;
+
+    case TLS_MODEL_INITIAL_EXEC:
+      /* tp-relative add */
+      tp = gen_rtx_REG (Pmode, XR17032_TP);
+      tmp = gen_reg_rtx (Pmode);
+      emit_insn (xr17032_got_load_tls_ie (tmp, loc));
+      dest = gen_reg_rtx (Pmode);
+      emit_insn (gen_add3_insn (dest, tmp, tp));
+      break;
+
+    case TLS_MODEL_LOCAL_EXEC:
+      tmp = xr17032_unspec_offset_high (NULL, loc, XR17032_SYMBOL_TLS_LE);
+      dest = gen_reg_rtx (Pmode);
+      emit_insn (xr17032_tls_add_tp_le (dest, tmp, loc));
+      dest = gen_rtx_LO_SUM (Pmode, dest,
+			     xr17032_unspec_address (loc, XR17032_SYMBOL_TLS_LE));
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+  return dest;
+}
+
+/* Returns the number of instructions necessary to reference a symbol. */
+
+static int xr17032_symbol_insns (enum xr17032_symbol_type type)
+{
+  switch (type)
+    {
+    case XR17032_SYMBOL_TLS: return 0; /* Depends on the TLS model.  */
+    case XR17032_SYMBOL_ABSOLUTE: return 2; /* LUI + the reference.  */
+    case XR17032_SYMBOL_PCREL: return 2; /* ADR + the reference.  */
+    case XR17032_SYMBOL_TLS_LE: return 3; /* LUI + ADD TP + the reference.  */
+    case XR17032_SYMBOL_GOT_DISP: return 3; /* ADR + LD GOT + the reference.  */
+    default: gcc_unreachable ();
+    }
+}
+
+/* Should a symbol of type SYMBOL_TYPE should be split in two?  */
+
+bool
+xr17032_split_symbol_type (enum xr17032_symbol_type symbol_type)
+{
+  if (symbol_type == XR17032_SYMBOL_TLS_LE)
+    return true;
+
+  return symbol_type == XR17032_SYMBOL_ABSOLUTE
+	 || symbol_type == XR17032_SYMBOL_PCREL;
+}
+
+/* Return true if a LO_SUM can address a value of mode MODE when the
+   LO_SUM symbol has type SYM_TYPE.  X is the LO_SUM second operand, which
+   is used when the mode is BLKmode.  */
+
+static bool
+xr17032_valid_lo_sum_p (enum xr17032_symbol_type sym_type, machine_mode mode,
+			rtx x)
+{
+  int align, size;
+
+  /* Check that symbols of type SYMBOL_TYPE can be used to access values
+     of mode MODE.  */
+  if (xr17032_symbol_insns (sym_type) == 0)
+    return false;
+
+  /* Check that there is a known low-part relocation.  */
+  if (!xr17032_split_symbol_type (sym_type))
+    return false;
+
+  /* We can't tell size or alignment when we have BLKmode, so try extracting a
+     decl from the symbol if possible.  */
+  if (mode == BLKmode)
+    {
+      rtx offset;
+
+      /* Extract the symbol from the LO_SUM operand, if any.  */
+      split_const (x, &x, &offset);
+
+      /* Might be a CODE_LABEL.  We can compute align but not size for that,
+	 so don't bother trying to handle it.  */
+      if (!SYMBOL_REF_P (x))
+	return false;
+
+      /* Use worst case assumptions if we don't have a SYMBOL_REF_DECL.  */
+      align = (SYMBOL_REF_DECL (x)
+	       ? DECL_ALIGN (SYMBOL_REF_DECL (x))
+	       : 1);
+      size = (SYMBOL_REF_DECL (x)
+	      && DECL_SIZE (SYMBOL_REF_DECL (x))
+	      && tree_fits_uhwi_p (DECL_SIZE (SYMBOL_REF_DECL (x)))
+	      ? tree_to_uhwi (DECL_SIZE (SYMBOL_REF_DECL (x)))
+	      : 2*BITS_PER_WORD);
+    }
+  else
+    {
+      align = GET_MODE_ALIGNMENT (mode);
+      size = GET_MODE_BITSIZE (mode);
+    }
+
+  /* We may need to split multiword moves, so make sure that each word
+     can be accessed without inducing a carry.  */
+  if (size > BITS_PER_WORD && size > align)
+    return false;
+
+  return true;
+}
+
+/* Classify the base of symbolic expression X.  */
+
+enum xr17032_symbol_type
+xr17032_classify_symbolic_expression (rtx x)
+{
+  rtx offset;
+
+  split_const (x, &x, &offset);
+  if (UNSPEC_ADDRESS_P (x))
+    return UNSPEC_ADDRESS_TYPE (x);
+
+  return xr17032_classify_symbol (x);
+}
+
+/* Return true if X is a valid address for machine mode MODE.  If it is,
+   fill in INFO appropriately.  STRICT_P is true if REG_OK_STRICT is in
+   effect.  */
+
+static bool
+xr17032_classify_address (struct xr17032_address_info *info, rtx x,
+			  machine_mode mode, bool strict_p)
+{
+  switch (GET_CODE (x))
+    {
+    case REG:
+    case SUBREG:
+      info->type = ADDRESS_REG;
+      info->reg = x;
+      info->offset = const0_rtx;
+      return xr17032_valid_base_register_p (info->reg, strict_p);
+
+    case PLUS:
+      info->type = ADDRESS_REG;
+      info->reg = XEXP (x, 0);
+      info->offset = XEXP (x, 1);
+
+      return (xr17032_valid_base_register_p (info->reg, strict_p)
+	      && xr17032_valid_offset_p (info->offset, mode));
+
+    case LO_SUM:
+      info->type = ADDRESS_LO_SUM;
+      info->reg = XEXP (x, 0);
+      info->offset = XEXP (x, 1);
+      /* We have to trust the creator of the LO_SUM to do something vaguely
+	 sane.  Target-independent code that creates a LO_SUM should also
+	 create and verify the matching HIGH.  Target-independent code that
+	 adds an offset to a LO_SUM must prove that the offset will not
+	 induce a carry.  Failure to do either of these things would be
+	 a bug, and we are not required to check for it here.  The XR/17032
+	 backend itself should only create LO_SUMs for valid symbolic
+	 constants, with the high part being either a HIGH or a copy
+	 of _gp. */
+      info->symbol_type
+	= xr17032_classify_symbolic_expression (info->offset);
+      return (xr17032_valid_base_register_p (info->reg, strict_p)
+	      && xr17032_valid_lo_sum_p (info->symbol_type, mode, info->offset));
+
+    case CONST_INT:
+      /* Small-integer addresses don't occur very often, but they
+	 are legitimate if x0 is a valid base register.  */
+      info->type = ADDRESS_CONST_INT;
+      return SMALL_OPERAND (INTVAL (x));
+
+    default:
+      return false;
+    }
+}
+
+/* If OP is an UNSPEC address, return the address to which it refers,
+   otherwise return OP itself.  */
+
+static rtx
+xr17032_strip_unspec_address (rtx op)
+{
+  rtx base, offset;
+
+  split_const (op, &base, &offset);
+  if (UNSPEC_ADDRESS_P (base))
+    op = plus_constant (Pmode, UNSPEC_ADDRESS (base), INTVAL (offset));
+  return op;
+}
+
+/* Print symbolic operand OP, which is part of a HIGH or LO_SUM
+   in context CONTEXT.  HI_RELOC indicates a high-part reloc.  */
+
+static void
+xr17032_print_operand_reloc (FILE *file, rtx op, bool hi_reloc)
+{
+  const char *reloc;
+
+  switch (xr17032_classify_symbolic_expression (op))
+    {
+      case XR17032_SYMBOL_ABSOLUTE:
+	reloc = hi_reloc ? "%hi" : "%lo";
+	break;
+
+      case XR17032_SYMBOL_PCREL:
+	reloc = hi_reloc ? "%pcrel_hi" : "%pcrel_lo";
+	break;
+
+      case XR17032_SYMBOL_TLS_LE:
+	reloc = hi_reloc ? "%tprel_hi" : "%tprel_lo";
+	break;
+
+      default:
+	output_operand_lossage ("invalid use of '%%%c'", hi_reloc ? 'h' : 'R');
+	return;
+    }
+
+  fprintf (file, "%s(", reloc);
+  output_addr_const (file, xr17032_strip_unspec_address (op));
+  fputc (')', file);
+}
+
+/* Implement TARGET_PRINT_OPERAND.  The XR/17032-specific operand codes are:
+
+   'h'	Print the high-part relocation associated with OP, after stripping
+	  any outermost HIGH.
+   'R'	Print the low-part relocation associated with OP.
+   'C'	Print the integer branch condition for comparison OP.
+   'r'	Print the inverse of the integer branch condition for comparison OP.
+   'z'	Print zero if OP is zero, otherwise print OP normally.
+   'i'	Print i if the operand is not a register.
+   'A'	Print the address of a memory operand.
+
+   Note please keep this list and the list in xr17032.md in sync.  */
+
+static void
+xr17032_print_operand (FILE *file, rtx op, int letter)
+{
+  machine_mode mode = GET_MODE (op);
+  enum rtx_code code = GET_CODE (op);
+
+  switch (letter)
+    {
+    case 'h':
+      if (code == HIGH)
+	op = XEXP (op, 0);
+      xr17032_print_operand_reloc (file, op, true);
+      break;
+
+    case 'R':
+      xr17032_print_operand_reloc (file, op, false);
+      break;
+
+    case 'C':
+      /* The RTL names match the instruction names. */
+      fputs (GET_RTX_NAME (code), file);
+      break;
+
+    case 'r':
+      /* The RTL names match the instruction names. */
+      fputs (GET_RTX_NAME (reverse_condition (code)), file);
+      break;
+
+    case 'i':
+      if (code != REG)
+        fputc ('i', file);
+      break;
+    default:
+      switch (code)
+	{
+	case REG:
+	  if (letter && letter != 'z')
+	    output_operand_lossage ("invalid use of '%%%c'", letter);
+	  fprintf (file, "%s", reg_names[REGNO (op)]);
+	  break;
+
+	case MEM:
+	  if (letter == 'A')
+	    xr17032_print_operand (file, XEXP (op, 0), 0);
+	  else if (letter && letter != 'z')
+	    output_operand_lossage ("invalid use of '%%%c'", letter);
+	  else
+	    output_address (mode, XEXP (op, 0));
+	  break;
+
+	default:
+	  if (letter == 'z' && op == CONST0_RTX (GET_MODE (op)))
+	    fputs ("zero", file);
+	  else if (letter && letter != 'z')
+	    output_operand_lossage ("invalid use of '%%%c'", letter);
+	  else
+	    {
+	      op = xr17032_strip_unspec_address (op);
+	      output_addr_const (file, op);
+	    }
+	  break;
+	}
+    }
+}
+
+static const char *
+xr17032_mode_id (machine_mode mode)
+{
+  switch (mode)
+    {
+    case QImode:
+      return "byte";
+    case HImode:
+      return "int";
+    case SImode:
+      return "long";
+    default:
+      internal_error ("internal error: bad memory mode: %d", mode);
+      return "(invalid)";
+    }
+}
+
+static void
+xr17032_print_operand_address (FILE *file, machine_mode mode, rtx x)
+{
+  struct xr17032_address_info addr;
+
+  fprintf (file, "%s ", xr17032_mode_id (mode));
+
+  if (xr17032_classify_address (&addr, x, word_mode, true))
+    switch (addr.type)
+      {
+      case ADDRESS_REG:
+	fprintf (file, "[%s + ", reg_names[REGNO (addr.reg)]);
+	output_addr_const (file, xr17032_strip_unspec_address (addr.offset));
+	fputc (']', file);
+	return;
+
+      case ADDRESS_LO_SUM:
+	fprintf (file, "[%s + ", reg_names[REGNO (addr.reg)]);
+	xr17032_print_operand_reloc (file, addr.offset, false);
+	fputc (']', file);
+	return;
+
+      case ADDRESS_CONST_INT:
+	fputs ("[zero + ", file);
+	output_addr_const (file, x);
+	fputc (']', file);
+	return;
+
+      case ADDRESS_SYMBOLIC:
+	output_addr_const (file, xr17032_strip_unspec_address (x));
+	return;
+
+      default:
+	gcc_unreachable ();
+      }
+
+  debug_rtx (x);
+  gcc_unreachable ();
+}
+
+/* Implement TARGET_LEGITIMATE_ADDRESS_P.  */
+
+static bool
+xr17032_legitimate_address_p (machine_mode mode, rtx x, bool strict_p,
+			      code_helper = ERROR_MARK)
+{
+  struct xr17032_address_info addr;
+
+  return xr17032_classify_address (&addr, x, mode, strict_p);
+}
+
+/* If X is not a valid address for mode MODE, force it into a register.  */
+
+static rtx
+xr17032_force_address (rtx x, machine_mode mode)
+{
+  if (!xr17032_legitimate_address_p (mode, x, false))
+    {
+      gcc_assert (can_create_pseudo_p ());
+      return force_reg (Pmode, x);
+    }
+
+  return x;
+}
+
+/* Emit an instruction of the form (set TARGET SRC).  */
+
+static rtx
+xr17032_emit_set (rtx target, rtx src)
+{
+  emit_insn (gen_rtx_SET (target, src));
+  return target;
+}
+
+/* Emit an instruction of the form (set DEST (CODE X Y)).  */
+
+static rtx
+xr17032_emit_binary (enum rtx_code code, rtx dest, rtx x, rtx y)
+{
+  return xr17032_emit_set (dest, gen_rtx_fmt_ee (code, GET_MODE (dest), x, y));
+}
+
+/* Helper for xr17032_legitimize_address. Given X, return true if it
+   is a left shift by 1 or 2 positions or a multiply by 2 or 4.
+
+   This respectively represent canonical shift-add rtxs or scaled
+   memory addresses.  */
+static bool
+mem_shadd_or_shadd_rtx_p (rtx x)
+{
+  return ((GET_CODE (x) == ASHIFT
+	   || GET_CODE (x) == MULT)
+	  && register_operand (XEXP (x, 0), GET_MODE (x))
+	  && CONST_INT_P (XEXP (x, 1))
+	  && ((GET_CODE (x) == ASHIFT && IN_RANGE (INTVAL (XEXP (x, 1)), 1, 2))
+	      || (GET_CODE (x) == MULT
+		  && IN_RANGE (exact_log2 (INTVAL (XEXP (x, 1))), 1, 2))));
+}
+
+/* Return a legitimate address for REG + OFFSET.  TEMP is as for
+   xr17032_force_temporary; it is only needed when OFFSET is not a
+   SMALL_OPERAND.  */
+
+static rtx
+xr17032_add_offset (rtx temp, rtx reg, HOST_WIDE_INT offset)
+{
+  if (!SMALL_OPERAND (offset))
+    {
+      rtx high;
+
+      /* Leave OFFSET as a 16-bit offset and put the excess in HIGH.
+	 The addition inside the macro CONST_HIGH_PART may cause an
+	 overflow, so we need to force a sign-extension check.  */
+      high = gen_int_mode (offset & ~(IMM_REACH - 1), Pmode);
+      offset = offset & (IMM_REACH - 1);
+      high = xr17032_force_temporary (temp, high);
+      reg = xr17032_force_temporary (temp, gen_rtx_PLUS (Pmode, high, reg));
+    }
+  return plus_constant (Pmode, reg, offset);
+}
+
+/* This function is used to implement LEGITIMIZE_ADDRESS.  If X can
+   be legitimized in a way that the generic machinery might not expect,
+   return a new address, otherwise return NULL.  MODE is the mode of
+   the memory being accessed.  */
+
+static rtx
+xr17032_legitimize_address (rtx x, rtx oldx ATTRIBUTE_UNUSED,
+			    machine_mode mode)
+{
+  rtx addr;
+
+  if (xr17032_tls_symbol_p (x))
+    return xr17032_legitimize_tls_address (x);
+
+  /* See if the address can split into a high part and a LO_SUM.  */
+  if (xr17032_split_symbol (NULL, x, mode, &addr))
+    return xr17032_force_address (addr, mode);
+
+  /* Handle BASE + OFFSET.  */
+  if (GET_CODE (x) == PLUS && CONST_INT_P (XEXP (x, 1))
+      && INTVAL (XEXP (x, 1)) != 0)
+    {
+      rtx base = XEXP (x, 0);
+      HOST_WIDE_INT offset = INTVAL (XEXP (x, 1));
+
+      /* Handle (plus (plus (mult (a) (mem_shadd_constant)) (fp)) (C)) case.  */
+      if (GET_CODE (base) == PLUS && mem_shadd_or_shadd_rtx_p (XEXP (base, 0))
+	  && SMALL_OPERAND (offset))
+	{
+	  rtx index = XEXP (base, 0);
+	  rtx fp = XEXP (base, 1);
+	  if (REG_P (fp) && REGNO (fp) == VIRTUAL_STACK_VARS_REGNUM)
+	    {
+	      /* If we were given a MULT, we must fix the constant
+		 as we're going to create the ASHIFT form.  */
+	      int shift_val = INTVAL (XEXP (index, 1));
+	      if (GET_CODE (index) == MULT)
+		shift_val = exact_log2 (shift_val);
+
+	      rtx reg1 = gen_reg_rtx (Pmode);
+	      rtx reg2 = gen_reg_rtx (Pmode);
+	      rtx reg3 = gen_reg_rtx (Pmode);
+	      xr17032_emit_binary (PLUS, reg1, fp, GEN_INT (offset));
+	      xr17032_emit_binary (ASHIFT, reg2, XEXP (index, 0), GEN_INT (shift_val));
+	      xr17032_emit_binary (PLUS, reg3, reg2, reg1);
+
+	      return reg3;
+	    }
+	}
+
+      if (!xr17032_valid_base_register_p (base, false))
+	base = copy_to_mode_reg (Pmode, base);
+      addr = xr17032_add_offset (NULL, base, offset);
+      return xr17032_force_address (addr, mode);
+    }
+
+  return x;
+}
+
+/* Return the number of instructions needed to load or store a value
+   of mode MODE at address X.  Return 0 if X isn't valid for MODE.
+   Assume that multiword moves may need to be split into word moves
+   if MIGHT_SPLIT_P, otherwise assume that a single load or store is
+   enough. */
+
+static int
+xr17032_address_insns (rtx x, machine_mode mode, bool might_split_p)
+{
+  struct xr17032_address_info addr = {};
+  int n = 1;
+
+  if (!xr17032_classify_address (&addr, x, mode, false))
+    {
+      return 3;
+    }
+
+  /* BLKmode is used for single unaligned loads and stores and should
+     not count as a multiword mode. */
+  if (mode != BLKmode && might_split_p)
+    n += (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+
+  if (addr.type == ADDRESS_LO_SUM)
+    n += xr17032_symbol_insns (addr.symbol_type) - 1;
+
+  return n;
+}
+
+/* Return the number of instructions needed to implement INSN,
+   given that it loads from or stores to MEM. */
+
+int
+xr17032_load_store_insns (rtx mem, rtx_insn *insn ATTRIBUTE_UNUSED)
+{
+  machine_mode mode;
+  bool might_split_p;
+
+  gcc_assert (MEM_P (mem));
+  mode = GET_MODE (mem);
+
+  /* Try to prove that INSN does not need to be split.  */
+  might_split_p = GET_MODE_BITSIZE (mode) > 32;
+
+  return xr17032_address_insns (XEXP (mem, 0), mode, might_split_p);
+}
+
+/* If MODE is MAX_MACHINE_MODE, ADDR appears as a move operand, otherwise
+   it appears in a MEM of that mode.  Return true if ADDR is a legitimate
+   constant in that context and can be split into high and low parts.
+   If so, and if LOW_OUT is nonnull, emit the high part and store the
+   low part in *LOW_OUT.  Leave *LOW_OUT unchanged otherwise.
+
+   TEMP is as for xr17032_force_temporary and is used to load the high
+   part into a register.
+
+   When MODE is MAX_MACHINE_MODE, the low part is guaranteed to be
+   a legitimize SET_SRC for an .md pattern, otherwise the low part
+   is guaranteed to be a legitimate address for mode MODE.  */
+
+bool
+xr17032_split_symbol (rtx temp, rtx addr, machine_mode mode, rtx *low_out)
+{
+  enum xr17032_symbol_type symbol_type;
+
+  if ((GET_CODE (addr) == HIGH && mode == MAX_MACHINE_MODE)
+      || !xr17032_symbolic_constant_p (addr, &symbol_type)
+      || xr17032_symbol_insns (symbol_type) == 0
+      || !xr17032_split_symbol_type (symbol_type))
+    return false;
+
+  if (low_out)
+    switch (symbol_type)
+      {
+      case XR17032_SYMBOL_ABSOLUTE:
+	{
+	  rtx high = gen_rtx_HIGH (Pmode, copy_rtx (addr));
+	  high = xr17032_force_temporary (temp, high);
+	  *low_out = gen_rtx_LO_SUM (Pmode, high, addr);
+	}
+	break;
+
+      case XR17032_SYMBOL_PCREL:
+	{
+	  static unsigned seqno;
+	  char buf[32];
+	  rtx label;
+
+	  ssize_t bytes = snprintf (buf, sizeof (buf), ".LA%u", seqno);
+	  gcc_assert ((size_t) bytes < sizeof (buf));
+
+	  label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
+	  SYMBOL_REF_FLAGS (label) |= SYMBOL_FLAG_LOCAL;
+	  /* ??? Ugly hack to make weak symbols work.  May need to change the
+	     RTL for the adr and/or low patterns to get a better fix for
+	     this.  */
+	  if (! nonzero_address_p (addr))
+	    SYMBOL_REF_WEAK (label) = 1;
+
+	  if (temp == NULL)
+	    temp = gen_reg_rtx (Pmode);
+
+	  emit_insn (gen_xr17032_adr (temp, copy_rtx (addr), GEN_INT (seqno)));
+
+	  *low_out = gen_rtx_LO_SUM (Pmode, temp, label);
+
+	  seqno++;
+	}
+	break;
+
+      default:
+	gcc_unreachable ();
+      }
+
+  return true;
+}
+
+/* Subroutine of xr17032_legitimize_move.  Move constant SRC into register
+   DEST given that SRC satisfies immediate_operand but doesn't satisfy
+   move_operand.  */
+
+static void
+xr17032_legitimize_const_move (machine_mode mode, rtx dest, rtx src)
+{
+  rtx base, offset;
+
+  /* Split moves of big integers into smaller pieces.  */
+  if (xr17032_splittable_const_int_operand (src, mode))
+    {
+      xr17032_move_integer (dest, dest, INTVAL (src));
+      return;
+    }
+
+  /* Split moves of symbolic constants into high/low pairs.  */
+  if (xr17032_split_symbol (dest, src, MAX_MACHINE_MODE, &src))
+    {
+      xr17032_emit_set (dest, src);
+      return;
+    }
+
+  /* Generate the appropriate access sequences for TLS symbols.  */
+  if (xr17032_tls_symbol_p (src))
+    {
+      xr17032_emit_move (dest, xr17032_legitimize_tls_address (src));
+      return;
+    }
+
+  /* If we have (const (plus symbol offset)), and that expression cannot
+     be forced into memory, load the symbol first and add in the offset.  Also
+     prefer to do this even if the constant _can_ be forced into memory, as it
+     usually produces better code.  */
+  split_const (src, &base, &offset);
+  if (offset != const0_rtx
+      && (targetm.cannot_force_const_mem (mode, src) || can_create_pseudo_p ()))
+    {
+      base = xr17032_force_temporary (dest, base);
+      xr17032_emit_move (dest, xr17032_add_offset (NULL, base, INTVAL (offset)));
+      return;
+    }
+
+  src = force_const_mem (mode, src);
+
+  /* Constant pool references are sometimes not legitimate addresses.  */
+  xr17032_split_symbol (dest, XEXP (src, 0), mode, &XEXP (src, 0));
+  xr17032_emit_move (dest, src);
+}
+
+/* If (set DEST SRC) is not a valid move instruction, emit an equivalent
+   sequence that is valid.  */
+
+bool
+xr17032_legitimize_move (machine_mode mode, rtx dest, rtx src)
+{
+  /* Expand
+       (set (reg:QI target) (mem:QI (address)))
+     to
+       (set (reg:DI temp) (zero_extend:DI (mem:QI (address))))
+       (set (reg:QI target) (subreg:QI (reg:DI temp) 0))
+     with auto-sign/zero extend.  */
+  if (GET_MODE_CLASS (mode) == MODE_INT
+      && GET_MODE_SIZE (mode) < UNITS_PER_WORD
+      && can_create_pseudo_p ()
+      && MEM_P (src))
+    {
+      rtx temp_reg;
+      int zero_extend_p;
+
+      temp_reg = gen_reg_rtx (word_mode);
+      zero_extend_p = (LOAD_EXTEND_OP (mode) == ZERO_EXTEND);
+      emit_insn (gen_extend_insn (temp_reg, src, word_mode, mode,
+				  zero_extend_p));
+      xr17032_emit_move (dest, gen_lowpart (mode, temp_reg));
+      return true;
+    }
+
+  if (!register_operand (dest, mode) && !xr17032_reg_or_0_operand (src, mode))
+    {
+      rtx reg;
+
+      if (GET_CODE (src) == CONST_INT)
+	{
+	  /* Apply the equivalent of PROMOTE_MODE here for constants to
+	     improve cse.  */
+	  machine_mode promoted_mode = mode;
+	  if (GET_MODE_CLASS (mode) == MODE_INT
+	      && GET_MODE_SIZE (mode) < UNITS_PER_WORD)
+	    promoted_mode = word_mode;
+
+	  if (xr17032_splittable_const_int_operand (src, mode))
+	    {
+	      reg = gen_reg_rtx (promoted_mode);
+	      xr17032_move_integer (reg, reg, INTVAL (src));
+	    }
+	  else
+	    reg = force_reg (promoted_mode, src);
+
+	  if (promoted_mode != mode)
+	    reg = gen_lowpart (mode, reg);
+	}
+      else
+	reg = force_reg (mode, src);
+      xr17032_emit_move (dest, reg);
+      return true;
+    }
+
+  /* We need to deal with constants that would be legitimate
+     immediate_operands but aren't legitimate move_operands.  */
+  if (CONSTANT_P (src) && !xr17032_move_operand (src, mode))
+    {
+      xr17032_legitimize_const_move (mode, dest, src);
+      set_unique_reg_note (get_last_insn (), REG_EQUAL, copy_rtx (src));
+      return true;
+    }
+
+  if (MEM_P (dest) && !xr17032_legitimate_address_p (mode, XEXP (dest, 0),
+						     reload_completed))
+    {
+      XEXP (dest, 0) = xr17032_force_address (XEXP (dest, 0), mode);
+    }
+
+  if (MEM_P (src) && !xr17032_legitimate_address_p (mode, XEXP (src, 0),
+						    reload_completed))
+    {
+      XEXP (src, 0) = xr17032_force_address (XEXP (src, 0), mode);
+    }
+
+  return false;
+}
+
+/* Return the appropriate instructions to move SRC into DEST.  Assume
+   that SRC is operand 1 and DEST is operand 0.  */
+
+const char *
+xr17032_output_move (rtx dest, rtx src)
+{
+  enum rtx_code dest_code, src_code;
+  machine_mode mode;
+
+  dest_code = GET_CODE (dest);
+  src_code = GET_CODE (src);
+  mode = GET_MODE (dest);
+
+  if (dest_code == REG)
+    {
+      if (src_code == MEM)
+	return "mov\t%0,%1";
+
+      if (src_code == CONST_INT)
+	{
+	  if (SMALL_OPERAND (INTVAL (src)))
+	    return "addi\t%0,zero,%1";
+
+	  if (SMALL_OPERAND (-INTVAL (src)))
+	    return "subi\t%0,zero,-(%1)";
+
+	  if (LUI_OPERAND (INTVAL (src)))
+	    return "lui\t%0,zero,%1";
+
+	  /* Should never reach here.  */
+	  abort ();
+	}
+
+      if (src_code == HIGH)
+	return "lui\t%0,zero,%h1";
+
+      if (xr17032_symbolic_operand (src, VOIDmode))
+	switch (xr17032_classify_symbolic_expression (src))
+	  {
+	  case XR17032_SYMBOL_GOT_DISP: return "1:\n\t"
+					       "adr\t%0,%%got_pcrel_hi(%1)\n\t"
+					       "mov\t%0,long [%0 + %%pcrel_lo(1b)]";
+	  case XR17032_SYMBOL_ABSOLUTE: return "lui\t%0,zero,%%hi(%1)\n\t"
+					       "ori\t%0,%0,%%lo(%1)";
+	  case XR17032_SYMBOL_PCREL: return "1:\n\t"
+					    "adr\t%0,%%pcrel_hi(%1)\n\t"
+					    "addi\t%0,%0,%%pcrel_lo(1b)";
+	  default: gcc_unreachable ();
+	  }
+    }
+  if ((src_code == REG) || (src == CONST0_RTX (mode)))
+    {
+      if (dest_code == REG)
+	return "add\t%0,%z1,zero";
+      if (dest_code == MEM)
+	return "mov\t%0,%z1";
+    }
+  gcc_unreachable ();
+}
+
+static bool
+xr17032_int_single_mov (HOST_WIDE_INT value)
+{
+  return IN_RANGE (value, -IMM_REACH, IMM_REACH) || LUI_OPERAND (value);
+}
+
+/* Load VALUE into DEST.  TEMP is as for xr17032_force_temporary.  */
+
+void
+xr17032_move_integer (rtx temp, rtx dest, HOST_WIDE_INT value)
+{
+  rtx x;
+
+  if (xr17032_int_single_mov (value))
+    x = GEN_INT (value);
+  else
+    {
+      x = xr17032_emit_set (temp, GEN_INT (value & ~(IMM_REACH - 1)));
+      x = gen_rtx_IOR (GET_MODE (dest), temp, GEN_INT (value & (IMM_REACH - 1)));
+    }
+
+  xr17032_emit_set (dest, x);
+}
+
+enum memmodel
+xr17032_union_memmodel (enum memmodel a, enum memmodel b)
+{
+  enum memmodel weaker = a <= b ? a : b;
+  enum memmodel stronger = a > b ? a : b;
+
+  switch (stronger)
+    {
+      case MEMMODEL_SEQ_CST:
+      case MEMMODEL_ACQ_REL:
+	return stronger;
+      case MEMMODEL_RELEASE:
+	if (weaker == MEMMODEL_ACQUIRE || weaker == MEMMODEL_CONSUME)
+	  return MEMMODEL_ACQ_REL;
+	else
+	  return stronger;
+      case MEMMODEL_ACQUIRE:
+      case MEMMODEL_CONSUME:
+      case MEMMODEL_RELAXED:
+	return stronger;
+      default:
+	gcc_unreachable ();
+    }
+}
+
+/* Given memory reference MEM, expand code to compute the aligned
+   memory address, shift and mask values and store them into
+   *ALIGNED_MEM, *SHIFT, *MASK and *NOT_MASK.  */
+
+void
+xr17032_subword_address (rtx mem, rtx *aligned_mem, rtx *shift, rtx *mask,
+		       rtx *not_mask)
+{
+  /* Align the memory address to a word.  */
+  rtx addr = force_reg (Pmode, XEXP (mem, 0));
+
+  rtx addr_offset = gen_reg_rtx (Pmode);
+  emit_move_insn (addr_offset, gen_rtx_AND (Pmode, addr,
+                                            gen_int_mode (3, SImode)));
+
+  rtx aligned_addr = gen_reg_rtx (Pmode);
+  emit_move_insn (aligned_addr, gen_rtx_MINUS (Pmode, addr, addr_offset));
+
+  *aligned_mem = change_address (mem, SImode, aligned_addr);
+
+  /* Calculate the shift amount.  */
+  emit_move_insn (*shift, gen_rtx_ASHIFT (SImode, addr_offset,
+					  gen_int_mode (3, SImode)));
+
+  /* Calculate the mask.  */
+  int unshifted_mask = GET_MODE_MASK (GET_MODE (mem));
+
+  emit_move_insn (*mask, gen_int_mode (unshifted_mask, SImode));
+
+  emit_move_insn (*mask, gen_rtx_ASHIFT (SImode, *mask, *shift));
+
+  emit_move_insn (*not_mask, gen_rtx_NOT (SImode, *mask));
+}
+
+/* Leftshift a subword within an SImode register.  */
+
+void
+xr17032_lshift_subword (machine_mode mode ATTRIBUTE_UNUSED, rtx value, rtx shift,
+			rtx *shifted_value)
+{
+  rtx value_reg = gen_reg_rtx (SImode);
+  emit_move_insn (value_reg, gen_lowpart (SImode, value));
+  emit_move_insn (*shifted_value, gen_rtx_ASHIFT (SImode, value_reg, shift));
+}
+
+/* Return the number of instructions needed to load constant X.
+   Return 0 if X isn't a valid constant.  */
+
+int
+xr17032_const_insns (rtx x)
+{
+  enum xr17032_symbol_type symbol_type;
+  rtx offset;
+
+  switch (GET_CODE (x))
+    {
+    case HIGH:
+      if (!xr17032_symbolic_constant_p (XEXP (x, 0), &symbol_type)
+	  || !xr17032_split_symbol_type (symbol_type))
+	return 0;
+
+      /* This is simply an LUI.  */
+      return 1;
+
+    case CONST_INT:
+      return !xr17032_int_single_mov (INTVAL (x)) + 1;
+
+    case CONST:
+      /* See if we can refer to X directly.  */
+      if (xr17032_symbolic_constant_p (x, &symbol_type))
+	return xr17032_symbol_insns (symbol_type);
+
+      /* Otherwise try splitting the constant into a base and offset.  */
+      split_const (x, &x, &offset);
+      if (offset != 0)
+	{
+	  int n = xr17032_const_insns (x);
+	  if (n != 0)
+	    return n + !xr17032_int_single_mov (INTVAL (offset)) + 1;
+	}
+      return 0;
+
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return xr17032_symbol_insns (xr17032_classify_symbol (x));
+
+    default:
+      return 0;
+    }
+}
+
+/* Implement TARGET_LEGITIMATE_CONSTANT_P.  */
+
+static bool
+xr17032_legitimate_constant_p (machine_mode mode ATTRIBUTE_UNUSED, rtx x)
+{
+  return xr17032_const_insns (x) > 0;
+}
+
+/* Implement TARGET_CANNOT_FORCE_CONST_MEM.
+   Return true if X cannot (or should not) be spilled to the
+   constant pool.  */
+
+static bool
+xr17032_cannot_force_const_mem (machine_mode mode ATTRIBUTE_UNUSED, rtx x)
+{
+  enum xr17032_symbol_type type;
+  rtx base, offset;
+
+  /* There is no assembler syntax for expressing an address-sized
+     high part.  */
+  if (GET_CODE (x) == HIGH)
+    return true;
+
+  split_const (x, &base, &offset);
+  if (xr17032_symbolic_constant_p (base, &type))
+    {
+      /* As an optimization, don't spill symbolic constants that are as
+	 cheap to rematerialize as to access in the constant pool.  */
+      if (SMALL_OPERAND (INTVAL (offset)) && xr17032_symbol_insns (type) > 0)
+	return true;
+
+      /* As an optimization, avoid needlessly generate dynamic relocations.  */
+      if (flag_pic)
+	return true;
+    }
+
+  /* TLS symbols must be computed by xr17032_legitimize_move.  */
+  if (tls_referenced_p (x))
+    return true;
+
+  return false;
+}
+
+#undef TARGET_OPTION_OVERRIDE
+#define TARGET_OPTION_OVERRIDE xr17032_option_override
+
+#undef TARGET_LEGITIMATE_ADDRESS_P
+#define TARGET_LEGITIMATE_ADDRESS_P xr17032_legitimate_address_p
+
+#undef TARGET_PRINT_OPERAND
+#define TARGET_PRINT_OPERAND xr17032_print_operand
+#undef TARGET_PRINT_OPERAND_ADDRESS
+#define TARGET_PRINT_OPERAND_ADDRESS xr17032_print_operand_address
+
+#undef TARGET_FRAME_POINTER_REQUIRED
+#define TARGET_FRAME_POINTER_REQUIRED xr17032_frame_pointer_required
+
+#undef TARGET_EXPAND_BUILTIN_VA_START
+#define TARGET_EXPAND_BUILTIN_VA_START xr17032_expand_builtin_va_start
+#undef TARGET_MUST_PASS_IN_STACK
+#define TARGET_MUST_PASS_IN_STACK must_pass_in_stack_var_size
+#undef TARGET_FUNCTION_ARG
+#define TARGET_FUNCTION_ARG xr17032_function_arg
+#undef TARGET_FUNCTION_ARG_ADVANCE
+#define TARGET_FUNCTION_ARG_ADVANCE xr17032_function_arg_advance
+#undef TARGET_FUNCTION_ARG_BOUNDARY
+#define TARGET_FUNCTION_ARG_BOUNDARY xr17032_function_arg_boundary
+
+#undef TARGET_FUNCTION_VALUE
+#define TARGET_FUNCTION_VALUE xr17032_function_value
+#undef TARGET_LIBCALL_VALUE
+#define TARGET_LIBCALL_VALUE xr17032_libcall_value
+#undef TARGET_FUNCTION_VALUE_REGNO_P
+#define TARGET_FUNCTION_VALUE_REGNO_P xr17032_function_value_regno_p
+#undef TARGET_RETURN_IN_MEMORY
+#define TARGET_RETURN_IN_MEMORY xr17032_return_in_memory
+
+#undef TARGET_HAVE_TLS
+#define TARGET_HAVE_TLS true
+
+#undef TARGET_CANNOT_COPY_INSN_P
+#define TARGET_CANNOT_COPY_INSN_P xr17032_cannot_copy_insn_p
+
+#undef TARGET_LEGITIMIZE_ADDRESS
+#define TARGET_LEGITIMIZE_ADDRESS xr17032_legitimize_address
+
+#undef TARGET_LEGITIMATE_CONSTANT_P
+#define TARGET_LEGITIMATE_CONSTANT_P xr17032_legitimate_constant_p
+
+#undef TARGET_CANNOT_FORCE_CONST_MEM
+#define TARGET_CANNOT_FORCE_CONST_MEM xr17032_cannot_force_const_mem
+
+struct gcc_target targetm = TARGET_INITIALIZER;
+
+#include "gt-xr17032.h"
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/xr17032.h gcc-workdir/gcc/config/xr17032/xr17032.h
--- gcc-clean/gcc/config/xr17032/xr17032.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/xr17032.h
@@ -0,0 +1,280 @@
+/* Target Definitions for XR/17032.
+   Copyright (C) 2025-2025 Free Software Foundation, Inc.
+   Contributed by monkuous.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#ifndef GCC_XR17032_H
+#define GCC_XR17032_H
+
+#define TARGET_CPU_CPP_BUILTINS() 	\
+  { 					\
+    builtin_define_std ("xr17032");	\
+  }
+
+#define REGISTER_NAMES {				\
+  "zero", "t0", "t1", "t2", "t3", "t4", "t5", "a0",	\
+  "a1", "a2", "a3", "s0", "s1", "s2", "s3", "s4",	\
+  "s5", "s6", "s7", "s8", "s9", "s10", "s11", "s12",	\
+  "s13", "s14", "s15", "s16", "s17", "tp", "sp", "lr",	\
+  "frame", "arg" }
+
+#define XR17032_ZERO 0
+#define XR17032_T0 1
+#define XR17032_T1 2
+#define XR17032_T2 3
+#define XR17032_T3 4
+#define XR17032_T4 5
+#define XR17032_T5 6
+#define XR17032_A0 7
+#define XR17032_A1 8
+#define XR17032_A2 9
+#define XR17032_A3 10
+#define XR17032_S0 11
+#define XR17032_S1 12
+#define XR17032_S2 13
+#define XR17032_S3 14
+#define XR17032_S4 15
+#define XR17032_S5 16
+#define XR17032_S6 17
+#define XR17032_S7 18
+#define XR17032_S8 19
+#define XR17032_S9 20
+#define XR17032_S10 21
+#define XR17032_S11 22
+#define XR17032_S12 23
+#define XR17032_S13 24
+#define XR17032_S14 25
+#define XR17032_S15 26
+#define XR17032_S16 27
+#define XR17032_S17 28
+#define XR17032_TP 29
+#define XR17032_SP 30
+#define XR17032_LR 31
+#define XR17032_FRAME 32
+#define XR17032_ARG 33
+
+#define FIRST_PSEUDO_REGISTER 34
+
+#define STACK_POINTER_REGNUM		XR17032_SP
+#define FRAME_POINTER_REGNUM		XR17032_FRAME
+#define ARG_POINTER_REGNUM		XR17032_ARG
+#define HARD_FRAME_POINTER_REGNUM 	XR17032_S0
+
+#define XR17032_PROLOGUE_TEMP_REGNUM (XR17032_T0)
+#define XR17032_PROLOGUE_TEMP(MODE) gen_rtx_REG (MODE, XR17032_PROLOGUE_TEMP_REGNUM)
+
+#define FUNCTION_ARG_REGNO_P(r) ((r) >= XR17032_A0 && (r) <= XR17032_A3)
+
+#define ELIMINABLE_REGS	{				\
+  { ARG_POINTER_REGNUM, STACK_POINTER_REGNUM },		\
+  { ARG_POINTER_REGNUM, HARD_FRAME_POINTER_REGNUM },	\
+  { FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM },	\
+  { FRAME_POINTER_REGNUM, HARD_FRAME_POINTER_REGNUM } }
+
+enum reg_class
+{
+  NO_REGS,
+  GENERAL_REGS,
+  ALL_REGS,
+  LIM_REG_CLASSES
+};
+
+#define REGNO_REG_CLASS(R) (((R) < XR17032_FRAME) ? GENERAL_REGS : ALL_REGS)
+
+#define REG_CLASS_CONTENTS {					\
+  { 0x00000000, 0x00000000 },  /* Empty */			\
+  { 0xFFFFFFFF, 0x00000000 },  /* General registers */		\
+  { 0xFFFFFFFF, 0x00000003 } } /* All registers */
+
+#define N_REG_CLASSES LIM_REG_CLASSES
+
+#define REG_CLASS_NAMES {	\
+  "NO_REGS",			\
+  "GENERAL_REGS",		\
+  "ALL_REGS" }
+
+/* unwritable: zero, tp, sp, lr, frame, arg */
+#define FIXED_REGISTERS {	\
+  1, 0, 0, 0, 0, 0, 0, 0,	\
+  0, 0, 0, 0, 0, 0, 0, 0,	\
+  0, 0, 0, 0, 0, 0, 0, 0,	\
+  0, 0, 0, 0, 0, 1, 1, 1,	\
+  1, 1 }
+
+/* call-clobbered: t0-t5, a0-a3, lr */
+#define CALL_REALLY_USED_REGISTERS {	\
+  0, 1, 1, 1, 1, 1, 1, 1,		\
+  1, 1, 1, 0, 0, 0, 0, 0,		\
+  0, 0, 0, 0, 0, 0, 0, 0,		\
+  0, 0, 0, 0, 0, 0, 0, 1,		\
+  0, 0 }
+
+#define BASE_REG_CLASS GENERAL_REGS
+#define INDEX_REG_CLASS GENERAL_REGS
+
+#ifndef REG_OK_STRICT
+#define REGNO_OK_FOR_BASE_P(X) \
+  xr17032_regno_ok_for_base_p ((X), 0)
+#else
+#define REGNO_OK_FOR_BASE_P(X) \
+  xr17032_regno_ok_for_base_p ((X), 1)
+#endif
+
+#define REGNO_OK_FOR_INDEX_P REGNO_OK_FOR_BASE_P
+
+#define UNITS_PER_WORD 4
+
+#define MOVE_MAX 4
+
+#define Pmode SImode
+#define FUNCTION_MODE SImode
+#define CASE_VECTOR_MODE SImode
+
+#define TRAMPOLINE_SIZE 12
+#define TRAMPOLINE_ALIGNMENT 32
+
+#define STRICT_ALIGNMENT 1
+
+#define FUNCTION_BOUNDARY 32
+
+#define BITS_BIG_ENDIAN 0
+#define BYTES_BIG_ENDIAN 0
+#define WORDS_BIG_ENDIAN 0
+
+#define BIGGEST_ALIGNMENT 32
+
+#define MAX_REGS_PER_ADDRESS 1
+
+#define STACK_BOUNDARY 32
+#define PARM_BOUNDARY 32
+
+#define SLOW_BYTE_ACCESS 1
+
+#define FUNCTION_PROFILER(FILE,LABELNO) (abort (), 0)
+
+#define ASM_OUTPUT_ALIGN(STREAM,POWER) \
+  fprintf (STREAM, "\t.p2align\t%d\n", POWER);
+
+#define FIRST_PARM_OFFSET(F) 0
+
+#define INITIAL_ELIMINATION_OFFSET(FROM, TO, OFFSET) \
+  (OFFSET) = xr17032_initial_elimination_offset (FROM, TO)
+
+#define STACK_DYNAMIC_OFFSET(FUNDECL) \
+  (crtl->outgoing_args_size + STACK_POINTER_OFFSET)
+
+#define STACK_GROWS_DOWNWARD 1
+#define FRAME_GROWS_DOWNWARD 1
+
+/* Define the standard XR/17032 calling convention and variants.  */
+
+typedef struct {
+  /* Number of integer registers used so far, up to MAX_ARGS_IN_REGISTERS. */
+  unsigned int num_gprs;
+} CUMULATIVE_ARGS;
+
+/* Initialize a variable CUM of type CUMULATIVE_ARGS
+   for a call to a function whose data type is FNTYPE.
+   For a library call, FNTYPE is 0.  */
+
+#define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, INDIRECT, N_NAMED_ARGS) \
+  xr17032_init_cumulative_args (&(CUM), (FNTYPE), (LIBNAME), (INDIRECT),   \
+			(N_NAMED_ARGS) != -1)
+
+#define ACCUMULATE_OUTGOING_ARGS 1
+#define RETURN_ADDR_RTX(COUNT, FRAMEADDR) xr17032_return_addr (COUNT)
+#define INCOMING_RETURN_ADDR_RTX gen_rtx_REG (VOIDmode, XR17032_LR)
+#define DWARF_FRAME_RETURN_COLUMN XR17032_LR
+#define DWARF_ALT_FRAME_RETURN_COLUMN 38
+#define MAX_ARGS_IN_REGISTERS 4
+#define DEFAULT_PCC_STRUCT_RETURN 0
+
+#define DEFAULT_SIGNED_CHAR 0
+
+#define GLOBAL_ASM_OP 			"\t.globl\t"
+#define TEXT_SECTION_ASM_OP		"\t.text"
+#define DATA_SECTION_ASM_OP		"\t.data"
+#define READONLY_DATA_SECTION_ASM_OP	"\t.section\t.rodata"
+#define BSS_SECTION_ASM_OP		"\t.bss"
+
+#ifndef ASM_APP_ON
+#define ASM_APP_ON " #APP\n"
+#endif
+
+#ifndef ASM_APP_OFF
+#define ASM_APP_OFF " #NO_APP\n"
+#endif
+
+#define IMM_BITS 16
+#define IMM_REACH (1LL << IMM_BITS)
+
+#define SMALL_OPERAND(VALUE) \
+  ((unsigned HOST_WIDE_INT) (VALUE) < IMM_REACH)
+
+#define SMALL_OPERAND_SIGNED(VALUE) \
+  ((unsigned HOST_WIDE_INT) (VALUE) + IMM_REACH/2 < IMM_REACH)
+
+#define LUI_OPERAND(VALUE)						\
+  (((VALUE) | ((1UL<<31) - IMM_REACH)) == ((1UL<<31) - IMM_REACH)	\
+   || ((VALUE) | ((1UL<<31) - IMM_REACH)) + IMM_REACH == 0)
+
+/* True if a VALUE (constant) can be expressed as sum of two U16 constants
+   (in range 0 to 65535).
+   Range check logic:
+     from: min U16 + 1
+       to: two times the min U16 value (to max out U16 bits).  */
+
+#define SUM_OF_TWO_U16(VALUE) \
+  (((VALUE) >= (IMM_REACH + 1)) && ((VALUE) <= (IMM_REACH * 2)))
+
+/* Variant with first value 4 byte aligned if involving stack regs.  */
+#define SUM_OF_TWO_U16_ALGN(VALUE) \
+  (((VALUE) >= ((IMM_REACH & ~3) + 1)) && ((VALUE) <= ((IMM_REACH & ~3) * 2)))
+
+#define SHIFT_COUNT_TRUNCATED 1
+#define LOAD_EXTEND_OP(MODE) ZERO_EXTEND
+
+#define PROMOTE_MODE(MODE, UNSIGNEDP, TYPE)     \
+  if (GET_MODE_CLASS (MODE) == MODE_INT         \
+      && GET_MODE_SIZE (MODE) < UNITS_PER_WORD) \
+    {                                           \
+      if ((MODE) == SImode)                     \
+        (UNSIGNEDP) = 1;                        \
+      (MODE) = word_mode;                       \
+    }
+
+#define WORD_REGISTER_OPERATIONS 1
+
+/* Describe how we implement __builtin_eh_return.  */
+#define EH_RETURN_DATA_REGNO(N) \
+  ((N) < 4 ? (N) + XR17032_T0 : INVALID_REGNUM)
+
+#define ASM_PREFERRED_EH_DATA_FORMAT(CODE,GLOBAL) \
+  (((GLOBAL) ? DW_EH_PE_indirect : 0) | DW_EH_PE_pcrel | DW_EH_PE_sdata4)
+
+#undef SIZE_TYPE
+#define SIZE_TYPE "unsigned int"
+
+#undef PTRDIFF_TYPE
+#define PTRDIFF_TYPE "int"
+
+/* Define this macro if it is as good or better to call a constant
+   function address than to call an address kept in a register.  */
+#define NO_FUNCTION_CSE	1
+
+#endif /* GCC_XR17032_H */
diff -urN --no-dereference gcc-clean/gcc/config/xr17032/xr17032.md gcc-workdir/gcc/config/xr17032/xr17032.md
--- gcc-clean/gcc/config/xr17032/xr17032.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/gcc/config/xr17032/xr17032.md
@@ -0,0 +1,727 @@
+;; Machine description for XR/17032
+;; Copyright (C) 2025-2025 Free Software Foundation, Inc.
+;; Contributed by monkuous
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published
+;; by the Free Software Foundation; either version 3, or (at your
+;; option) any later version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT
+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+;; License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(include "predicates.md")
+(include "constraints.md")
+
+(define_mode_iterator QIHI [QI HI])
+(define_mode_iterator HISI [HI SI])
+(define_mode_iterator ANYI [QI HI SI])
+
+(define_code_iterator any_atomic [plus ior xor and])
+
+(define_code_attr atomic_optab
+  [(plus "add") (ior "or") (xor "xor") (and "and")])
+(define_code_attr insn
+  [(plus "add") (ior "or") (xor "xor") (and "and")])
+
+(define_attr "type"
+  "other,load,store,branch,jump,shift_shift"
+  (const_string "other"))
+
+;; Length of instruction in bytes.
+(define_attr "length" ""
+   (cond [
+	  ;; Branches further than +/- 4 MiB require three instructions.
+	  (eq_attr "type" "branch")
+	  (if_then_else (and (le (minus (match_dup 1) (pc))
+				 (const_int 4194303))
+			     (le (minus (pc) (match_dup 1))
+				 (const_int 4194304)))
+			(const_int 4)
+			(const_int 12))
+
+	  ;; Jumps further than +/- 4 MiB require two instructions.
+	  (eq_attr "type" "jump")
+	  (if_then_else (and (le (minus (match_dup 0) (pc))
+				 (const_int 4194303))
+			     (le (minus (pc) (match_dup 0))
+				 (const_int 4194304)))
+			(const_int 4)
+			(const_int 8))
+
+	  ;; SHIFT_SHIFTs are decomposed into two separate instructions.
+	  (eq_attr "type" "shift_shift") (const_int 8)
+
+	  ;; Otherwise, constants, loads and stores are handled by external
+	  ;; routines.
+	  (eq_attr "type" "load")
+	  (symbol_ref "xr17032_load_store_insns (operands[1], insn) * 4")
+	  (eq_attr "type" "store")
+	  (symbol_ref "xr17032_load_store_insns (operands[0], insn) * 4")
+	  ] (const_int 4)))
+
+;; Is copying of this instruction disallowed?
+(define_attr "cannot_copy" "no,yes" (const_string "no"))
+
+;; Keep this list and the one above xr17032_print_operand in sync.
+;; The special asm out single letter directives following a '%' are:
+;; h -- Print the high-part relocation associated with OP, after stripping
+;;	  any outermost HIGH.
+;; R -- Print the low-part relocation associated with OP.
+;; C -- Print the integer branch condition for comparison OP.
+;; r -- Print the inverse of the integer branch condition for comparison OP.
+;; z -- Print zero if OP is zero, otherwise print OP normally.
+;; i -- Print i if the operand is not a register.
+;; A -- Print the address of a memory operand.
+(define_c_enum "unspec" [
+  ;; Symbolic accesses.  The order of this list must match that of
+  ;; enum xr17032_symbol_type in xr17032-protos.h.
+  UNSPEC_ADDRESS_FIRST
+  UNSPEC_PCREL
+  UNSPEC_LOAD_GOT
+  UNSPEC_TLS
+  UNSPEC_TLS_LE
+  UNSPEC_TLS_IE
+  UNSPEC_TLS_GD
+
+  UNSPEC_ADR
+  UNSPEC_TIE
+])
+
+;; -------------------------------------------------------------------------
+;; Prologue & Epilogue
+;; -------------------------------------------------------------------------
+
+(define_expand "prologue"
+  [(const_int 1)]
+  ""
+{
+  xr17032_expand_prologue ();
+  DONE;
+})
+
+(define_expand "epilogue"
+  [(const_int 2)]
+  ""
+{
+  xr17032_expand_epilogue (XR17032_EPILOGUE_NORMAL);
+  DONE;
+})
+
+(define_insn "xr17032_return"
+  [(return)
+   (use (reg:SI 31))]
+  "reload_completed"
+  "jalr\tzero,lr,0")
+
+(define_insn "xr17032_stack_tie"
+  [(set (mem:BLK (scratch))
+	(unspec:BLK [(match_operand:SI 0 "register_operand" "r")
+		     (match_operand:SI 1 "register_operand" "r")]
+		    UNSPEC_TIE))]
+  "!rtx_equal_p (operands[0], operands[1])"
+  ""
+  [(set_attr "length" "0")]
+)
+
+;; This is used in compiling the unwind routines.
+(define_expand "eh_return"
+  [(use (match_operand 0 "general_operand"))]
+  ""
+{
+  if (GET_MODE (operands[0]) != word_mode)
+    operands[0] = convert_to_mode (word_mode, operands[0], 0);
+
+  emit_insn (gen_xr17032_eh_set_lr (operands[0]));
+  emit_jump_insn (gen_xr17032_eh_return_internal ());
+  emit_barrier ();
+  DONE;
+})
+
+(define_insn "xr17032_eh_set_lr"
+  [(match_operand:SI 0 "register_operand" "r")
+   (clobber (match_scratch:SI 1 "=&r"))]
+  ""
+  "#")
+
+(define_insn_and_split "xr17032_eh_return_internal"
+  [(eh_return)]
+  ""
+  "#"
+  "epilogue_completed"
+  [(const_int 0)]
+  "xr17032_expand_epilogue (XR17032_EPILOGUE_EH_RETURN); DONE;")
+
+;; -------------------------------------------------------------------------
+;; nop instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "nop"
+  [(const_int 0)]
+  ""
+  "addi\tzero,zero,0")
+
+;; -------------------------------------------------------------------------
+;; mov instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "xr17032_got_load"
+   [(set (match_operand:SI      0 "register_operand" "=r")
+	 (unspec:SI
+	     [(match_operand:SI 1 "xr17032_symbolic_operand" "")]
+	     UNSPEC_LOAD_GOT))]
+  ""
+  "1:\;adr\t%0,%%got_pcrel_hi(%1)\;mov\t%0,long [%0 + %%pcrel_lo(1b)]"
+   [(set_attr "length" "8")])
+
+(define_insn "xr17032_tls_add_tp_le"
+  [(set (match_operand:SI      0 "register_operand" "=r")
+	(unspec:SI
+	    [(match_operand:SI 1 "register_operand" "r")
+	     (match_operand:SI 2 "register_operand" "r")
+	     (match_operand:SI 3 "xr17032_symbolic_operand" "")]
+	    UNSPEC_TLS_LE))]
+  ""
+  "add\t%0,%1,%2"
+  [(set_attr "length" "8")])
+
+(define_insn "xr17032_got_load_tls_gd"
+  [(set (match_operand:SI      0 "register_operand" "=r")
+	(unspec:SI
+	    [(match_operand:SI 1 "xr17032_symbolic_operand" "")]
+	    UNSPEC_TLS_GD))]
+  ""
+  "1:\;adr\t%0,%%tls_gd_pcrel_hi(%1)\;addi\t%0,%0,%%pcrel_lo(1b)"
+  [(set_attr "length" "8")])
+
+(define_insn "xr17032_got_load_tls_ie"
+  [(set (match_operand:SI      0 "register_operand" "=r")
+	(unspec:SI
+	    [(match_operand:SI 1 "xr17032_symbolic_operand" "")]
+	    UNSPEC_TLS_IE))]
+  ""
+  "1:\;adr\t%0,%%tls_ie_pcrel_hi(%1)\;mov\t%0,long [%0 + %%pcrel_lo(1b)]"
+  [(set_attr "length" "8")])
+
+(define_insn "xr17032_adr"
+  [(set (match_operand:SI           0 "register_operand" "=r")
+	(unspec:SI
+	    [(match_operand:SI      1 "xr17032_symbolic_operand" "")
+		  (match_operand:SI 2 "const_int_operand")
+		  (pc)]
+	    UNSPEC_ADR))]
+  ""
+  ".LA%2:\;adr\t%0,%h1"
+  [(set_attr "cannot_copy" "yes")])
+
+;; Instructions for adding the low 16 bits of an address to a register.
+;; Operand 2 is the address: xr17032_print_operand works out which relocation
+;; should be applied.
+
+(define_insn "*lowsi"
+  [(set (match_operand:SI            0 "register_operand" "=r")
+	(lo_sum:SI (match_operand:SI 1 "register_operand" " r")
+		   (match_operand:SI 2 "xr17032_symbolic_operand" "")))]
+  ""
+  "addi\t%0,%1,%R2")
+
+;; Allow combine to split complex const_int load sequences, using operand 2
+;; to store the intermediate results.  See move_operand for details.
+(define_split
+  [(set (match_operand:SI     0 "register_operand")
+	(match_operand:SI     1 "xr17032_splittable_const_int_operand"))
+   (clobber (match_operand:SI 2 "register_operand"))]
+  ""
+  [(const_int 0)]
+{
+  xr17032_move_integer (operands[2], operands[0], INTVAL (operands[1]));
+  DONE;
+})
+
+;; Likewise, for symbolic operands.
+(define_split
+  [(set (match_operand:SI 0 "register_operand")
+	(match_operand:SI 1))
+   (clobber (match_operand:SI 2 "register_operand"))]
+  "xr17032_split_symbol (operands[2], operands[1], MAX_MACHINE_MODE, NULL)"
+  [(set (match_dup 0) (match_dup 3))]
+{
+  xr17032_split_symbol (operands[2], operands[1],
+			MAX_MACHINE_MODE, &operands[3]);
+})
+
+;; 32-bit Integer moves
+
+(define_expand "movsi"
+  [(set (match_operand:SI 0 "")
+	(match_operand:SI 1 ""))]
+  ""
+{
+  if (xr17032_legitimize_move (SImode, operands[0], operands[1]))
+    DONE;
+})
+
+(define_insn "*movsi_internal"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,r, m")
+	(match_operand:SI 1 "xr17032_move_operand" "rT,m,rO"))]
+  "(register_operand (operands[0], SImode)
+    || xr17032_reg_or_0_operand (operands[1], SImode))"
+  { return xr17032_output_move (operands[0], operands[1]); }
+  [(set_attr "type" "other,load,store")])
+
+;; 16-bit Integer moves
+
+;; Unlike most other insns, the move insns can't be split with
+;; different predicates, because register spilling and other parts of
+;; the compiler, have memoized the insn number already.
+
+(define_expand "movhi"
+  [(set (match_operand:HI 0 "")
+	(match_operand:HI 1 ""))]
+  ""
+{
+  if (xr17032_legitimize_move (HImode, operands[0], operands[1]))
+    DONE;
+})
+
+(define_insn "*movhi_internal"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=r,r, m")
+	(match_operand:HI 1 "xr17032_move_operand" "rT,m,rO"))]
+  "(register_operand (operands[0], HImode)
+    || xr17032_reg_or_0_operand (operands[1], HImode))"
+  { return xr17032_output_move (operands[0], operands[1]); }
+  [(set_attr "type" "other,load,store")])
+
+;; 8-bit Integer moves
+
+(define_expand "movqi"
+  [(set (match_operand:QI 0 "")
+	(match_operand:QI 1 ""))]
+  ""
+{
+  if (xr17032_legitimize_move (QImode, operands[0], operands[1]))
+    DONE;
+})
+
+(define_insn "*movqi_internal"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=r,r, m")
+	(match_operand:QI 1 "xr17032_move_operand" "rT,m,rO"))]
+  "(register_operand (operands[0], QImode)
+    || xr17032_reg_or_0_operand (operands[1], QImode))"
+  { return xr17032_output_move (operands[0], operands[1]); }
+  [(set_attr "type" "other,load,store")])
+
+;; -------------------------------------------------------------------------
+;; add instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "addsi3"
+  [(set (match_operand:SI          0 "register_operand"         "=r")
+	(plus:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		 (match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "add%i2\t%0,%z1,%2")
+
+;; -------------------------------------------------------------------------
+;; sub instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "subsi3"
+  [(set (match_operand:SI           0 "register_operand"         "=r")
+	(minus:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		  (match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "sub%i2\t%0,%z1,%2")
+
+;; -------------------------------------------------------------------------
+;; and instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "andsi3"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+	(and:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		(match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "and%i2\t%0,%z1,%2")
+
+;; -------------------------------------------------------------------------
+;; xor instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "xorsi3"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+	(xor:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		(match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "xor%i2\t%0,%z1,%2")
+
+(define_insn "one_cmplsi2"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+	(not:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")))]
+  ""
+  "nor\t%0,%z1,zero")
+
+;; -------------------------------------------------------------------------
+;; or instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "iorsi3"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+	(ior:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		(match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "or%i2\t%0,%z1,%2")
+
+;; -------------------------------------------------------------------------
+;; lsh instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "ashlsi3"
+  [(set (match_operand:SI            0 "register_operand"         "=r,r,r")
+	(ashift:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" " r,r,O")
+		   (match_operand:SI 2 "xr17032_shift_operand"    " r,L,rL")))]
+  ""
+  "@
+   lsh\t%0,%1,%2
+   add\t%0,zero,%1 LSH %2
+   add\t%0,zero,zero")
+
+;; -------------------------------------------------------------------------
+;; rsh instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "lshrsi3"
+  [(set (match_operand:SI              0 "register_operand"         "=r,r,r")
+	(lshiftrt:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" " r,r,O")
+		     (match_operand:SI 2 "xr17032_shift_operand"    " r,L,rL")))]
+  ""
+  "@
+   rsh\t%0,%1,%2
+   add\t%0,zero,%1 RSH %2
+   add\t%0,zero,zero")
+
+;; -------------------------------------------------------------------------
+;; ash instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "ashrsi3"
+  [(set (match_operand:SI              0 "register_operand"         "=r,r,r")
+	(ashiftrt:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" " r,r,O")
+		     (match_operand:SI 2 "xr17032_shift_operand"    " r,L,rL")))]
+  ""
+  "@
+   ash\t%0,%1,%2
+   add\t%0,zero,%1 ASH %2
+   add\t%0,zero,zero")
+
+;; -------------------------------------------------------------------------
+;; ror instruction
+;; -------------------------------------------------------------------------
+
+(define_expand "rotlsi3"
+  [(set (match_operand:SI              0 "register_operand"         "=r,r,r")
+	(rotatert:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" " r,r,O")
+		     (match_operand:SI 2 "xr17032_shift_operand"    " r,L,rL")))]
+  ""
+{
+  if (CONST_INT_P (operands[2]))
+    operands[2] = GEN_INT (32 - (INTVAL (operands[2]) % 32));
+  else
+    {
+      rtx reg = gen_reg_rtx (SImode);
+      emit_move_insn (reg, GEN_INT (32));
+      emit_insn (gen_subsi3 (reg, reg, operands[2]));
+      operands[2] = reg;
+    }
+})
+
+(define_insn "rotrsi3"
+  [(set (match_operand:SI              0 "register_operand"         "=r,r,r")
+	(rotatert:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" " r,r,O")
+		     (match_operand:SI 2 "xr17032_shift_operand"    " r,L,rL")))]
+  ""
+  "@
+   ror\t%0,%1,%2
+   add\t%0,zero,%1 ROR %2
+   add\t%0,zero,zero")
+
+;; -------------------------------------------------------------------------
+;; mul instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "mulsi3"
+  [(set (match_operand:SI          0 "register_operand"         "=r")
+        (mult:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+	         (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")))]
+  ""
+  "mul\t%0,%z1,%z2")
+
+;; -------------------------------------------------------------------------
+;; div instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "udivsi3"
+  [(set (match_operand:SI          0 "register_operand"         "=r")
+        (udiv:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+	         (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")))]
+  ""
+  "div\t%0,%z1,%z2")
+
+(define_insn "divsi3"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+        (div:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+	        (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")))]
+  ""
+  "div signed\t%0,%z1,%z2")
+
+;; -------------------------------------------------------------------------
+;; mod instruction
+;; -------------------------------------------------------------------------
+
+(define_insn "umodsi3"
+  [(set (match_operand:SI          0 "register_operand"         "=r")
+        (umod:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+	         (match_operand:SI 2 "xr17032_reg_or_0_operand" "rO")))]
+  ""
+  "mod\t%0,%z1,%z2")
+
+;; -------------------------------------------------------------------------
+;; jumps
+;; -------------------------------------------------------------------------
+
+(define_insn "jump"
+  [(set (pc)
+	(label_ref (match_operand 0 "" "")))]
+  ""
+{
+  if (get_attr_length (insn) == 8)
+    return "1:\;"
+	   "adr\tlr,%%pcrel_hi(%0)\;"
+	   "jalr\tzero,lr,%%pcrel_lo(1b)";
+
+  return "beq\tzero,%0";
+}
+  [(set_attr "type" "jump")])
+
+(define_insn "indirect_jump"
+  [(set (pc)
+	(match_operand:SI 0 "register_operand" "r"))]
+  ""
+  "jalr\tzero,%0,0")
+
+;; -------------------------------------------------------------------------
+;; jal instruction
+;; -------------------------------------------------------------------------
+
+(define_expand "call"
+  [(call (match_operand 0 "")
+	 (match_operand 1 ""))]
+  ""
+{
+  rtx target = XEXP (operands[0], 0);
+  if (!xr17032_call_operand (target, VOIDmode))
+    target = force_reg (SImode, target);
+  emit_call_insn (gen_xr17032_call (target, operands[1]));
+  DONE;
+})
+
+(define_insn "xr17032_call"
+  [(call (mem:SI (match_operand 0 "xr17032_call_operand" "r,Sa,Sp,Sg"))
+	 (match_operand 1 "" ""))
+   (clobber (reg:SI 31))]
+  ""
+  "@
+   jalr\tlr,%0,0
+   jal\t%0
+   1:\;adr\tlr,%%pcrel_hi(%0)\;jalr\tlr,lr,%%pcrel_lo(1b)
+   1:\;adr\tlr,%%plt_pcrel_hi(%0)\;jalr\tlr,lr,%%pcrel_lo(1b)"
+  [(set_attr "length" "4,4,8,8")])
+
+(define_expand "call_value"
+  [(set (match_operand 0 "")
+        (call (match_operand 1 "")
+	      (match_operand 2 "")))]
+  ""
+{
+  rtx target = XEXP (operands[1], 0);
+  if (!xr17032_call_operand (target, VOIDmode))
+    target = force_reg (SImode, target);
+  emit_call_insn (gen_xr17032_call_value (operands[0], target, operands[2]));
+  DONE;
+})
+
+(define_insn "xr17032_call_value"
+  [(set (match_operand 0 "" "")
+        (call (mem:SI (match_operand 1 "xr17032_call_operand" "r,Sa,Sp,Sg"))
+	      (match_operand 2 "" "")))
+   (clobber (reg:SI 31))]
+  ""
+  "@
+   jalr\tlr,%1,0
+   jal\t%1
+   1:\;adr\tlr,%%pcrel_hi(%1)\;jalr\tlr,lr,%%pcrel_lo(1b)
+   1:\;adr\tlr,%%plt_pcrel_hi(%1)\;jalr\tlr,lr,%%pcrel_lo(1b)"
+  [(set_attr "length" "4,4,8,8")])
+
+;; -------------------------------------------------------------------------
+;; conditionals
+;; -------------------------------------------------------------------------
+
+(define_code_iterator cond [eq ne lt gt ge le])
+(define_code_attr rcode [(eq "ne") (ne "eq")
+			 (lt "ge") (gt "le")
+			 (ge "lt") (le "gt")])
+
+(define_expand "cstoresi4"
+  [(set (match_operand:SI    0 "register_operand")
+	(match_operator:SI   1 "xr17032_slt_operator"
+			[(match_operand:SI 2 "xr17032_reg_or_0_operand")
+			 (match_operand:SI 3 "")]))]
+  ""
+{
+  if (GET_CODE (operands[1]) == LTU
+      && !xr17032_arith_operand (operands[3], SImode))
+    FAIL;
+  else if (GET_CODE (operands[1]) == LT
+	   && !xr17032_arith_operand_signed (operands[3], SImode))
+    FAIL;
+})
+
+(define_insn "xr17032_sltu"
+  [(set (match_operand:SI         0 "register_operand"         "=r")
+        (ltu:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+		(match_operand:SI 2 "xr17032_arith_operand"    "rI")))]
+  ""
+  "slt%i2\t%0,%z1,%2")
+
+(define_insn "xr17032_slt"
+  [(set (match_operand:SI        0 "register_operand"             "=r")
+	(lt:SI (match_operand:SI 1 "xr17032_reg_or_0_operand"     "rO")
+	       (match_operand:SI 2 "xr17032_arith_operand_signed" "rJ")))]
+  ""
+  "slt%i2 signed\t%0,%z1,%2")
+
+(define_insn "*set_eq_zero"
+  [(set (match_operand:SI        0 "register_operand"         "=r")
+        (eq:SI (match_operand:SI 1 "xr17032_reg_or_0_operand" "rO")
+	       (const_int 0)))]
+  ""
+  "slti\t%0,%z1,1")
+
+(define_expand "cbranchsi4"
+  [(set (pc)
+	(if_then_else (match_operator      0 "ordered_comparison_operator"
+			[(match_operand:SI 1 "xr17032_reg_or_0_operand")
+			 (match_operand:SI 2 "xr17032_arith_operand")])
+		      (label_ref (match_operand 3 ""))
+		      (pc)))]
+  ""
+{
+  int code = GET_CODE (operands[0]);
+
+  if (code == LTU || code == GTU || code == LEU || code == GEU)
+    {
+      rtx reg = gen_reg_rtx (SImode);
+      if (code == LTU || code == GEU)
+	emit_insn (gen_xr17032_sltu (reg, operands[1], operands[2]));
+      else
+	{
+	  if (!xr17032_reg_shift_operand (operands[2], SImode))
+	    operands[2] = force_reg (SImode, operands[2]);
+	  emit_insn (gen_xr17032_sltu (reg, operands[2], operands[1]));
+	}
+      PUT_CODE (operands[0], code == LTU || code == GTU ? NE : EQ);
+      operands[1] = reg;
+      operands[2] = CONST0_RTX (SImode);
+    }
+  else if (!CONST_INT_P (operands[2]) || INTVAL (operands[2]) != 0)
+    {
+      rtx reg = gen_reg_rtx (SImode);
+      emit_insn (gen_subsi3 (reg, operands[1], operands[2]));
+      operands[1] = reg;
+      operands[2] = CONST0_RTX (SImode);
+    }
+})
+
+(define_insn "*b<code>"
+  [(set (pc)
+        (if_then_else (cond (match_operand:SI 0 "xr17032_reg_or_0_operand" "rO")
+			    (const_int 0))
+		      (label_ref (match_operand 1 ""))
+		      (pc)))]
+  ""
+{
+  if (get_attr_length (insn) == 12)
+    return "b<rcode>\t%z0,2f\;"
+	   "1:\;"
+	   "adr\tlr,%%pcrel_hi(%1)\;"
+	   "jalr\tzero,lr,%%pcrel_lo(1b)\;"
+	   "2:";
+
+  return "b<code>\t%z0,%1";
+}
+  [(set_attr "type" "branch")])
+
+;; -------------------------------------------------------------------------
+;; trap
+;; -------------------------------------------------------------------------
+
+(define_insn "trap"
+  [(trap_if (const_int 1) (const_int 0))]
+  ""
+  "brk")
+
+;; -------------------------------------------------------------------------
+;; extension
+;; -------------------------------------------------------------------------
+
+(define_insn_and_split "extend<QIHI:mode><HISI:mode>2"
+  [(set (match_operand:HISI   0 "register_operand" "=r")
+	(sign_extend:HISI
+	    (match_operand:QIHI 1 "register_operand" " r")))]
+  ""
+  "#"
+  "&& reload_completed
+   && REG_P (operands[1])
+   && !paradoxical_subreg_p (operands[0])"
+  [(set (match_dup 0) (ashift:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (ashiftrt:SI (match_dup 0) (match_dup 2)))]
+{
+  operands[0] = gen_lowpart (SImode, operands[0]);
+  operands[1] = gen_lowpart (SImode, operands[1]);
+  operands[2] = GEN_INT (GET_MODE_BITSIZE (SImode)
+			 - GET_MODE_BITSIZE (<QIHI:MODE>mode));
+}
+  [(set_attr "type" "shift_shift")])
+
+(define_insn "zero_extendqi<mode>2"
+  [(set (match_operand:HISI                 0 "register_operand"     "=r,r")
+	(zero_extend:HISI (match_operand:QI 1 "nonimmediate_operand" " r,m")))]
+  ""
+  "@
+   andi\t%0,%1,0xff
+   mov\t%0,%1")
+
+(define_insn "zero_extendhisi2"
+  [(set (match_operand:SI                 0 "register_operand"     "=r,r")
+	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" " r,m")))]
+  ""
+  "@
+   andi\t%0,%1,0xffff
+   mov\t%0,%1")
+
+(define_expand "get_thread_pointersi"
+  [(set (match_operand:SI 0 "register_operand" "=r")
+	(reg:SI 29))]
+  ""
+  "")
+
+(include "sync.md")
diff -urN --no-dereference gcc-clean/gcc/config.gcc gcc-workdir/gcc/config.gcc
--- gcc-clean/gcc/config.gcc
+++ gcc-workdir/gcc/config.gcc
@@ -593,6 +593,10 @@
 	extra_headers="c6x_intrinsics.h"
 	extra_options="${extra_options} c6x/c6x-tables.opt"
 	;;
+xr17032-*-*)
+	cpu_type=xr17032
+	target_has_targetm_common=no
+	;;
 xtensa*-*-*)
 	extra_options="${extra_options} fused-madd.opt"
 	extra_objs="xtensa-dynconfig.o"
@@ -3606,6 +3610,16 @@
 	tm_file="elfos.h ${tm_file} visium/elf.h newlib-stdint.h"
 	tmake_file="visium/t-visium visium/t-crtstuff"
 	;;
+xr17032-*-elf*)
+	tm_file="elfos.h newlib-stdint.h ${tm_file}"
+	;;
+xr17032-*-linux*)
+	tm_file="elfos.h gnu-user.h linux.h glibc-stdint.h ${tm_file} xr17032/linux.h"
+	tm_defines="${tm_defines} TARGET_DEFAULT_ASYNC_UNWIND_TABLES=1"
+	gnu_ld=yes
+	gas=yes
+	gcc_cv_initfini_array=yes
+	;;
 xstormy16-*-elf)
 	# For historical reasons, the target files omit the 'x'.
 	tm_file="elfos.h newlib-stdint.h stormy16/stormy16.h"
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/crti.S gcc-workdir/libgcc/config/xr17032/crti.S
--- gcc-clean/libgcc/config/xr17032/crti.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/crti.S
@@ -0,0 +1,51 @@
+# Start .init and .fini sections.
+# Copyright (C) 2025-2025 Free Software Foundation, Inc.
+#
+# This file is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3, or (at your option)
+# any later version.
+#
+# GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+# WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# for more details.
+#
+# Under Section 7 of GPL version 3, you are granted additional
+# permissions described in the GCC Runtime Library Exception, version
+# 3.1, as published by the Free Software Foundation.
+#
+# You should have received a copy of the GNU General Public License and
+# a copy of the GCC Runtime Library Exception along with this program;
+# see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+# <http://www.gnu.org/licenses/>.
+
+# This file just makes a stack frame for the contents of the .fini and
+# .init sections.  Users may put any desired instructions in those
+# sections.
+
+/* An executable stack is *not* required for these functions.  */
+#if defined(__ELF__) && defined(__linux__)
+.section .note.GNU-stack,"",%progbits
+.previous
+#endif
+
+	.section .init
+	.globl _init
+	.type _init,@function
+	.align	4
+_init:
+	subi	sp,sp,8
+	mov	long [sp],s0
+	mov	long [sp+4],lr
+	addi	s0,sp,0
+
+	.section .fini
+	.globl _fini
+	.type _fini,@function
+	.align	4
+_fini:
+	subi	sp,sp,8
+	mov	long [sp],s0
+	mov	long [sp+4],lr
+	addi	s0,sp,0
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/crtn.S gcc-workdir/libgcc/config/xr17032/crtn.S
--- gcc-clean/libgcc/config/xr17032/crtn.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/crtn.S
@@ -0,0 +1,44 @@
+# End of .init and .fini sections.
+# Copyright (C) 2025-2025 Free Software Foundation, Inc.
+#
+# This file is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3, or (at your option)
+# any later version.
+#
+# GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+# WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# for more details.
+#
+# Under Section 7 of GPL version 3, you are granted additional
+# permissions described in the GCC Runtime Library Exception, version
+# 3.1, as published by the Free Software Foundation.
+#
+# You should have received a copy of the GNU General Public License and
+# a copy of the GCC Runtime Library Exception along with this program;
+# see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+# <http://www.gnu.org/licenses/>.
+
+
+# This file just makes sure that the .fini and .init sections do in
+# fact return.  Users may put any desired instructions in those sections.
+# This file is the last thing linked into any executable.
+
+/* An executable stack is *not* required for these functions.  */
+#if defined(__ELF__) && defined(__linux__)
+.section .note.GNU-stack,"",%progbits
+.previous
+#endif
+
+	.section .init
+	mov	s0,long [sp]
+	mov	lr,long [sp+4]
+	addi	sp,sp,8
+	jalr	zero,lr,0
+
+	.section .fini
+	mov	s0,long [sp]
+	mov	lr,long [sp+4]
+	addi	sp,sp,8
+	jalr	zero,lr,0
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/linux-unwind.h gcc-workdir/libgcc/config/xr17032/linux-unwind.h
--- gcc-clean/libgcc/config/xr17032/linux-unwind.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/linux-unwind.h
@@ -0,0 +1,86 @@
+/* Copyright (C) 2025-2025 Free Software Foundation, Inc.
+
+   This file is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the
+   Free Software Foundation; either version 3, or (at your option) any
+   later version.
+
+   This file is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+#ifndef inhibit_libc
+
+#include <signal.h>
+#include <stdint.h>
+#include <sys/ucontext.h>
+
+#define ADDI_T5_ZERO_8B 0x008b01bc
+#define SYS             0x00000031
+
+#define MD_FALLBACK_FRAME_STATE_FOR xr17032_fallback_frame_state
+
+static _Unwind_Reason_Code
+xr17032_fallback_frame_state (struct _Unwind_Context *context,
+			      _Unwind_FrameState * fs)
+{
+  /* The kernel creates an rt_sigframe on the stack immediately prior
+     to delivering a signal.
+
+     This structure must have the same shape as the linux kernel
+     equivalent.  */
+  struct rt_sigframe
+  {
+    siginfo_t info;
+    ucontext_t uc;
+  };
+
+  struct rt_sigframe *rt_;
+  _Unwind_Ptr new_cfa;
+  uint32_t *pc = context->ra;
+  struct sigcontext *sc;
+  int i;
+
+  /* A signal frame will have a return address pointing to
+     __default_sa_restorer. This code is hardwired as:
+
+     0x008b01bc		addi	t5,zero,0x8b
+     0x00000031		sys
+   */
+  if (pc[0] != ADDI_T5_ZERO_8B || pc[1] != SYS)
+    return _URC_END_OF_STACK;
+
+  rt_ = context->cfa;
+  sc = (struct sigcontext *) &rt_->uc.uc_mcontext;
+
+  new_cfa = (_Unwind_Ptr) sc;
+  fs->regs.cfa_how = CFA_REG_OFFSET;
+  fs->regs.cfa_reg = __LIBGCC_STACK_POINTER_REGNUM__;
+  fs->regs.cfa_offset = new_cfa - (_Unwind_Ptr) context->cfa;
+
+  for (i = 0; i < 32; i++)
+    {
+      fs->regs.how[i] = REG_SAVED_OFFSET;
+      fs->regs.reg[i].loc.offset = (_Unwind_Ptr) &sc->gregs[i] - new_cfa;
+    }
+
+  fs->signal_frame = 1;
+  fs->retaddr_column = __LIBGCC_DWARF_ALT_FRAME_RETURN_COLUMN__;
+  fs->regs.how[fs->retaddr_column] = REG_SAVED_VAL_OFFSET;
+  fs->regs.reg[fs->retaddr_column].loc.offset =
+    (_Unwind_Ptr) sc->gregs[0] - new_cfa;
+
+  return _URC_NO_REASON;
+}
+
+#endif
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/sfp-machine.h gcc-workdir/libgcc/config/xr17032/sfp-machine.h
--- gcc-clean/libgcc/config/xr17032/sfp-machine.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/sfp-machine.h
@@ -0,0 +1,95 @@
+/* Software floating-point machine description for XR/17032.
+
+   Copyright (C) 2025-2025 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+Under Section 7 of GPL version 3, you are granted additional
+permissions described in the GCC Runtime Library Exception, version
+3.1, as published by the Free Software Foundation.
+
+You should have received a copy of the GNU General Public License and
+a copy of the GCC Runtime Library Exception along with this program;
+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+<http://www.gnu.org/licenses/>.  */
+
+#define _FP_W_TYPE_SIZE		32
+#define _FP_W_TYPE		unsigned long
+#define _FP_WS_TYPE		signed long
+#define _FP_I_TYPE		long
+
+#define _FP_MUL_MEAT_S(R,X,Y)				\
+  _FP_MUL_MEAT_1_wide(_FP_WFRACBITS_S,R,X,Y,umul_ppmm)
+#define _FP_MUL_MEAT_D(R,X,Y)				\
+  _FP_MUL_MEAT_2_wide(_FP_WFRACBITS_D,R,X,Y,umul_ppmm)
+#define _FP_MUL_MEAT_Q(R,X,Y)				\
+  _FP_MUL_MEAT_4_wide(_FP_WFRACBITS_Q,R,X,Y,umul_ppmm)
+
+#define _FP_DIV_MEAT_S(R,X,Y)	_FP_DIV_MEAT_1_udiv_norm(S,R,X,Y)
+#define _FP_DIV_MEAT_D(R,X,Y)	_FP_DIV_MEAT_2_udiv(D,R,X,Y)
+#define _FP_DIV_MEAT_Q(R,X,Y)	_FP_DIV_MEAT_4_udiv(Q,R,X,Y)
+
+#define _FP_NANFRAC_B		_FP_QNANBIT_B
+#define _FP_NANFRAC_H		_FP_QNANBIT_H
+#define _FP_NANFRAC_S		_FP_QNANBIT_S
+#define _FP_NANFRAC_D		_FP_QNANBIT_D, 0
+#define _FP_NANFRAC_Q		_FP_QNANBIT_Q, 0, 0, 0
+
+/* The type of the result of a floating point comparison.  This must
+   match __libgcc_cmp_return__ in GCC for the target.  */
+typedef int __gcc_CMPtype __attribute__ ((mode (__libgcc_cmp_return__)));
+#define CMPtype __gcc_CMPtype
+
+#define _FP_NANSIGN_B		0
+#define _FP_NANSIGN_H		0
+#define _FP_NANSIGN_S		0
+#define _FP_NANSIGN_D		0
+#define _FP_NANSIGN_Q		0
+
+#define _FP_KEEPNANFRACP 0
+#define _FP_QNANNEGATEDP 0
+
+#define _FP_CHOOSENAN(fs, wc, R, X, Y, OP)	\
+  do {						\
+    R##_s = _FP_NANSIGN_##fs;			\
+    _FP_FRAC_SET_##wc(R,_FP_NANFRAC_##fs);	\
+    R##_c = FP_CLS_NAN;				\
+  } while (0)
+
+#define _FP_DECL_EX		int _frm __attribute__ ((unused));
+#define FP_ROUNDMODE		_frm
+
+#define FP_RND_NEAREST		0x0
+#define FP_RND_ZERO		0x1
+#define FP_RND_PINF		0x3
+#define FP_RND_MINF		0x2
+
+#define FP_EX_INVALID		0x10
+#define FP_EX_OVERFLOW		0x04
+#define FP_EX_UNDERFLOW		0x02
+#define FP_EX_DIVZERO		0x08
+#define FP_EX_INEXACT		0x01
+
+#define _FP_TININESS_AFTER_ROUNDING 1
+
+#define FP_INIT_ROUNDMODE	_frm = FP_RND_NEAREST
+
+#define	__LITTLE_ENDIAN	1234
+#define	__BIG_ENDIAN	4321
+#define __BYTE_ORDER __LITTLE_ENDIAN
+
+
+/* Define ALIASNAME as a strong alias for NAME.  */
+# define strong_alias(name, aliasname) _strong_alias(name, aliasname)
+# define _strong_alias(name, aliasname) \
+  extern __typeof (name) aliasname __attribute__ ((alias (#name)));
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/t-crtstuff gcc-workdir/libgcc/config/xr17032/t-crtstuff
--- gcc-clean/libgcc/config/xr17032/t-crtstuff	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/t-crtstuff
@@ -0,0 +1 @@
+CRTSTUFF_T_CFLAGS += -fno-asynchronous-unwind-tables -fno-unwind-tables
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/t-elf gcc-workdir/libgcc/config/xr17032/t-elf
--- gcc-clean/libgcc/config/xr17032/t-elf	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/t-elf
@@ -0,0 +1,2 @@
+# Avoid the full unwinder being pulled along with the division libcalls.
+LIB2_DIVMOD_EXCEPTION_FLAGS := -fasynchronous-unwind-tables
diff -urN --no-dereference gcc-clean/libgcc/config/xr17032/t-softfp gcc-workdir/libgcc/config/xr17032/t-softfp
--- gcc-clean/libgcc/config/xr17032/t-softfp	1970-01-01 01:00:00.000000000 +0100
+++ gcc-workdir/libgcc/config/xr17032/t-softfp
@@ -0,0 +1,6 @@
+softfp_int_modes := si di
+softfp_exclude_libgcc2 := n
+
+softfp_float_modes := sf df
+softfp_extensions := sfdf
+softfp_truncations := dfsf
diff -urN --no-dereference gcc-clean/libgcc/config.host gcc-workdir/libgcc/config.host
--- gcc-clean/libgcc/config.host
+++ gcc-workdir/libgcc/config.host
@@ -218,6 +218,9 @@
 tic6x-*-*)
 	cpu_type=c6x
 	;;
+xr17032-*-*)
+	cpu_type=xr17032
+	;;
 esac
 
 # Common parts for widely ported systems.
@@ -1543,6 +1546,15 @@
         extra_parts="$extra_parts crtbegin.o crtend.o crti.o crtn.o"
         tmake_file="visium/t-visium t-fdpbit"
         ;;
+xr17032-*-linux*)
+	tmake_file="${tmake_file} xr17032/t-crtstuff xr17032/t-softfp t-softfp xr17032/t-elf t-slibgcc-libgcc"
+	extra_parts="$extra_parts crtbegin.o crtend.o crti.o crtn.o crtendS.o crtbeginT.o"
+	md_unwind_header=xr17032/linux-unwind.h
+	;;
+xr17032-*-*)
+	tmake_file="${tmake_file} xr17032/t-softfp t-softfp xr17032/t-elf"
+	extra_parts="$extra_parts crtbegin.o crtend.o crti.o crtn.o"
+	;;
 xstormy16-*-elf)
 	tmake_file="stormy16/t-stormy16 t-fdpbit"
 	;;
